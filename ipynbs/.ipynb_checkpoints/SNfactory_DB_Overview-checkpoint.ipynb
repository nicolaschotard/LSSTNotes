{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SNfactory production plan flow\n",
    "The SNfactory production plan flow is composed by 10 major **plans**, producing all the data from prepocessed files to final flux-calibrated and host-subtracted spectra (found in the IDR). These plans are preceded by a few other scripts doing the data copy from the summit to the CC, and filling the DB with these new data and information. As you can see in the following figure, all the first steps (before photometric ratios estimate and flux calibration) are night-oriented (run on all targets of a given night), while most of the others are target-oriented (run on all nights of a given target). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Plan Flow](figures/planFlow.png)](figures/planFlow.pdf \"SNfactory plan flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is in order the list of scripts and plans that are run to produce the final SNfactory *science* data-set (the IDR) from the raw observations.\n",
    "\n",
    "Data transfer and summit cleaning:\n",
    "\n",
    "* `export_sync`: Run at summit. Make sure all data are sent from the summit to the CC IN2P3.\n",
    "* `hsi_import`: copy the new data to the HPSS disk.\n",
    "* summit cleaning: Clean the summit computer for new observations.\n",
    "\n",
    "DB header update and data filling:\n",
    "\n",
    "* `snf_header`: Update the DB header with the new informations.\n",
    "* `snf_db_make`: Create a pickle file containg all the new data info.\n",
    "* `snf_db_fill`: Fill the DB using the previoulsy made pickle file.\n",
    "\n",
    "Update DB from warehouse information (z, Ebmv) and flag Run **Kind** (e.g., for references):\n",
    "\n",
    "* `SyncTarget`: synchronize the new target information found in WareHouse with the DB information.\n",
    "* `FlagRunKind`: check the Run.Kind for errors or mismatches and update it if needed, e.g., to define if a spectrum is a final reference (according to its time difference with the previous observation), or to check for new SCALA data.\n",
    "\n",
    "Data processing:\n",
    "\n",
    "* `plan_file_quality`: data preprocessing.\n",
    "* `plan_synthetic_arcs`: include synthetic arcs in the DB.\n",
    "* `plan_cube_generation`: cube extraction.\n",
    "* `plan_extract_star`: point source object extraction and spectra production.\n",
    "* `plan_multi_standard`: multi-standard extinction estimate.\n",
    "* `tabPhotometricity`: compute and update the night photometricity.\n",
    "* `plan_photometric_ratios`: compute the photometric flux ratios for a given list of target.\n",
    "* `plan_photometric_ratios`: (scale factor): compute the MFR scale factor.\n",
    "* `plan_flux_solution`: compute the flux solutions in a given night. \n",
    "* `plan_flux_calibration`: flux calibrate the spectra and cubes.\n",
    "* `plan_analyze_timeseries`: merge spectra, compute magnitude, run SALT2 fits.\n",
    "* `plan_gs_psf`: PSF estimate from the photometric channel.\n",
    "* `plan_cubefit`: host-galaxy subtraction (plan_ddt is deprecated).\n",
    "* `plan_extract_star`: point-source object extraction on host-galaxy subtracted cubes.\n",
    "\n",
    "Final dataset pre-analysis and packaging:\n",
    "\n",
    "* `plan_analyse_timeseries`: same as before, but on host-subtracted data.\n",
    "* `study_flux_quality`: check the magnitude dispersion for StdStars and SALT2 fits for SNe Ia. Build the good/bad lists.\n",
    "* `update_idr_config`: use this script to create the CONFIG.yaml file from an old IDR.\n",
    "* `build_idr`: build an IDR from a given production name and list of targets.\n",
    "\n",
    "If not explicitely specified, all these scripts have to be run on the CC under the **snprod** account. Details on how to run (most of) them are given on the following SNf [twiki page](https://snf-doc.lbl.gov/twiki/bin/view/Tasks/NewDataProcessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File name convention\n",
    "\n",
    "If you have observed with SNIFS, you will know the raw-data filename format. There are deviations from this structure, but they are not numerous. The raw-data filename format is hierarchical: \n",
    "\n",
    "    YY_DDD_RRR_EEE_FF_C.ext\n",
    "    \n",
    "where\n",
    "\n",
    "* **YY**: 2-digit code for a year (UTC).\n",
    "* **DDD**: 3-digit code for day of year the file was created (UTC).\n",
    "* **RRR**: 3-digit \"run code\" -- starts at 001 every UTC day.\n",
    "    * A run is a temporally connected group of exposures made by SNIFS.\n",
    "    * A run consists of one or more exposures.\n",
    "* **EEE**: 3-digit \"exposure code\" -- starts at 001 for each run.\n",
    "    * Exposures may have been taken simultaneoustly in a run (across multiple channels of SNIFS: R, B or P).\n",
    "    * Exposures may have been taken in sequence in a run (a continuum, followed by a science exposure, followed by an arc).\n",
    "* **FF**: 2-digit \"fclass\" -- indicates what type of file this is.\n",
    "    * An exhaustive list of fclasses can be found at the CC by running SnfFclass at the command line (fclasses are in the first column of output).\n",
    "    * Just ignore the leading 0 for raw data files (see below).\n",
    "    * Some important ones familiar to SNIFS observers: 03 -- arc, 17 -- science, 07 -- continuum, etc.\n",
    "    * The pipeline will make use of fclasses that are potentially unfamiliar to someone who has used SNIFS and know.\n",
    "* **C**: 1-letter \"channel\" code -- one of R, B, or P.\n",
    "    * Tells what channel the exposure was taken on.\n",
    "\n",
    "In the SNIFS processing framework, there is an extended filename format for **processed data**. Once raw data are registered they receive an extended filename. The extended filename format is also hierarchical: \n",
    "\n",
    "    YY_DDD_RRR_EEE_C_FFF_XXX_VV-VV_III.ext (e.g. 06_334_103_004_4_004_107_02-02_000.fits)\n",
    "\n",
    "where\n",
    "\n",
    "* **YY**, **DDD**, **RRR**, **EEE** are same as above.\n",
    "* **C**: 1-digit \"channel\" code (note, in raw-data format it is a letter, here it is a digit).\n",
    "    * The code is a bitwise-and of multiple channel codes: P = 1, R = 2, B = 4.\n",
    "    * So, 6 is B+R, 7 is B+R+P, etc.\n",
    "* **FFF**: 3-digit \"fclass\" -- an extension to the raw-data fclass formats.\n",
    "    * An exhaustive list of fclasses can be found at the CC by running SnfFclass at the command line (fclasses are in the first column of output, see next section).\n",
    "    * Raw data have a 0 prepended onto their fclass, so fclass below 100 is \"reserved\" for raw data.\n",
    "    * Other files created by the pipeline will have fclasses greater than 099, and are assigned as needed.\n",
    "* **XXX**: 3-digit \"xfclass\" -- a further extension to the fclass framework.\n",
    "    * An exhaustive list of xfclasses can be found at the CC by running SnfFclass at the command line.\n",
    "    * Think of an fclass as a \"namespace\" so an xfclass can appear once per fclass, but multiple times across fclasses.\n",
    "    * This trick is often used to track production elements (a common xfclass indicates one route through a section of the pipeline).\n",
    "    * A given plan/script will produce different type of files allways having the same set of Fclass/XFclass.\n",
    "* **VV-VV**: 4-digit \"processing version\" (latest in use are (200, 201 and 203)\n",
    "    * This is supposed to be a tag representing the state of the software run to produce the file.\n",
    "* **III**: 3-digit \"index\"\n",
    "    * An integer index differentiating otherwise identically named files (usually, due to reprocessing or re-running the pipeline with changed inputs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnfFclass\n",
    "Each processed file is associated to a specific set of Fclass/XFclass, mostly given chronologically according to the plan flow. Raw data and preprocessed files have low Fclass numbers (respectively 17 and 18 for object cubes, see below), while flux calibrated spectra and SALT2 light-curve output files have a high Fclass numbers (666 and 700 respectively). All sets of Fclass/XFclass are available through the `SnfFclass.py` script, which is stored on the SNFactory CVS repository under `SNFactory/Tasks/Processing/database/SnfObj/`. It can be used directly from the shell or call in an ipython as shown below.\n",
    "\n",
    "## Basic Fclass / XFclass info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F    XF   Description\n",
      "017  ---  Raw object frame\n",
      "017  000  Raw object frame\n",
      "017  001  Guide star vid-file\n",
      "017  002  Guide star gs-file\n",
      "017  003  Guide star ot-file\n",
      "017  004  Guide star tg-file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import SnfFclass\n",
    "print SnfFclass.document(17) # SnfFclass -f 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F    XF   Description\n",
      "666  ---  Flux-calibrated spectrum\n",
      "666  000  Flux-calibrated spectrum - quick_extract\n",
      "666  001  Flux-calibrated variance - quick_extract\n",
      "666  010  Flux-calibrated background - quick_extract\n",
      "666  011  Flux-calibrated bkgnd variance - quick_extract\n",
      "666  100  Flux-calibrated spectrum - extract_star\n",
      "666  101  Flux-calibrated variance - extract_star\n",
      "666  110  Flux-calibrated background - extract_star\n",
      "666  111  Flux-calibrated bkgnd variance - extract_star\n",
      "666  632  DDT + extract_star correlation plot [png]\n",
      "666  720  Flux-calibrated spectrum - DDT + extract_star\n",
      "666  722  Flux-calibrated background - DDT + extract_star\n",
      "666  724  DDT + extract_star 2D-fit log-file\n",
      "666  725  DDT + extract_star 3D-fit log-file\n",
      "666  730  DDT + extract_star spectrum plot [png]\n",
      "666  731  DDT + extract_star slice fit plot [png]\n",
      "666  732  DDT + extract_star profile plot [png]\n",
      "666  733  DDT + extract_star ADR plot [png]\n",
      "666  734  DDT + extract_star residual plot [png]\n",
      "666  735  DDT + extract_star parameter plot [png]\n",
      "666  736  DDT + extract_star radial profile plot [png]\n",
      "666  737  DDT + extract_star PSF contour plot [png]\n",
      "666  800  Cubefit - Point-source spectrum - extract_star\n",
      "666  810  Cubefit - Background spectrum - extract_star\n",
      "666  820  Cubefit - Residual spectrum - extract_star\n",
      "666  830  Cubefit - extract_star spectrum plot [png]\n",
      "666  831  Cubefit - extract_star slice fit plot [png]\n",
      "666  832  Cubefit - extract_star profile plot [png]\n",
      "666  833  Cubefit - extract_star ADR plot [png]\n",
      "666  834  Cubefit - extract_star residual plot [png]\n",
      "666  835  Cubefit - extract_star parameter plot [png]\n",
      "666  836  Cubefit - extract_star radial profile plot [png]\n",
      "666  837  Cubefit - extract_star PSF contour plot [png]\n",
      "666  840  Cubefit - extract_star 2D-fit log-file\n",
      "666  841  Cubefit - extract_star 3D-fit log-file\n",
      "666  850  Cubefit - Flux-calibrated spectrum\n",
      "666  900  CubeSim true SN spectrum\n",
      "666  910  CubeSim true Sky spectrum\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print SnfFclass.document(666) # SnfFclass -f 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking for a Fclass/XFclass related to a specific kind of processed, SALT2 in the following example, use it this way:\n",
    "\n",
    "    SnfFclass | grep SALT2\n",
    "    679:700  ---  SALT2 fit files\n",
    "    680:700  000  SALT2 fit results YAML file          - quick_extract\n",
    "    681:700  001  SALT2 lightcurve fit plot            - quick_extract\n",
    "    682:700  002  SALT2 fit logfile                    - quick_extract\n",
    "    683:700  003  SALT2 fit results (meta) YAML file   - quick_extract\n",
    "    685:700  101  SALT2 lightcurve fit plot            - extract_star\n",
    "    684:700  100  SALT2 fit results YAML file          - extract_star\n",
    "    686:700  102  SALT2 fit logfile                    - extract_star\n",
    "    687:700  103  SALT2 fit results (meta) YAML file   - extract_star\n",
    "    688:700  720  SALT2 fit results YAML file          - DDT + extract_start\n",
    "    689:700  721  SALT2 lightcurve fit plot            - DDT + extract_start\n",
    "    690:700  722  SALT2 fit logfile                    - DDT + extract_start\n",
    "    691:700  723  SALT2 fit results (meta) YAML file   - DDT + extract_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parents / Children\n",
    "\n",
    "### Fclass / XFclass sets\n",
    "\n",
    "A process/file is produced by a plan, which gives to this process a specific set of Fclass/XFclass according to its kind. A process always has at least one **parent** from which it is derived, and is most of the time used to produce an other file, and has thus at least one **child**. This parent/child relation is represented in the following diagram, where all set of Fclass/XFclass have been included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![F/XFclass](figures/FXFclass.png)](figures/FXFclass.png \"Fclass/XFclass relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given set of Fclass/XFclass, you can also get the list of their parents/children FClass/XFclass with the *SnfFclass* script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parents of processes with Fclass=38, XFclass=100\n",
      "   F    XF      Description\n",
      "  022  000  Reduced object cube [Euro3D]\n"
     ]
    }
   ],
   "source": [
    "SnfFclass.get_parents(('38','100')) # SnfFclass -p 38,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children of processes with Fclass=38, XFclass=100\n",
      "   F    XF      Description\n",
      "  620  600  Multi-std telluric correction\n",
      "  625  600  Multi-std extinction (photometric)\n",
      "  625  610  Multi-std extinction (non-photometric)\n",
      "  630  600  Multi-std flux solution (photometric)\n",
      "  630  610  Multi-std flux solution (non-photometric)\n",
      "  630  700  MFR adjusted multi-std flux solution\n",
      "  640  100  Unknown XFclass. Fclass: Fx-calib spectrum (w/telluric) \n",
      "  640  110  Unknown XFclass. Fclass: Fx-calib spectrum (w/telluric) \n"
     ]
    }
   ],
   "source": [
    "SnfFclass.get_children(('38','100')) # SnfFclass -c 38,100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs\n",
    "\n",
    "If you have a job or a set of jobs in mind (e.g., SNF-0203-NEWYORKf, or more specificaly SNF-0203-NEWYORKf-09dlc), and would like to know what are their typical input and output files, use the following function of SnfFclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main inputs:\n",
      "* 22 / 0: Reduced object cube [Euro3D]\n",
      "* 38 / 100: Point-source spectrum - extract_star\n",
      "* 620 / 600: Multi-std telluric correction\n",
      "* 625 / 600: Multi-std extinction (photometric)\n",
      "* 625 / 700: MFR adjusted multi-std extinction\n",
      "* 630 / 600: Multi-std flux solution (photometric)\n",
      "* 630 / 700: MFR adjusted multi-std flux solution\n",
      "* 995 / 0: SkyProbe photometricity data\n",
      "* 995 / 42: Photometricity overrides\n",
      "Main outputs:\n",
      "* 23 / 0: Flux-calibrated cube [Euro3D]\n",
      "* 23 / 10: Telluric-corrected, flux-calibrated cube [Euro3D]\n",
      "* 23 / 12: Telluric-corrected, flux-calibrated cube [3D]\n",
      "* 640 / 100: Fx-calib spectrum (w/telluric)\n",
      "* 640 / 110: Fx-calib spectrum (w/telluric)\n",
      "* 666 / 100: Flux-calibrated spectrum - extract_star\n",
      "* 666 / 110: Flux-calibrated background - extract_star\n"
     ]
    }
   ],
   "source": [
    "SnfFclass.print_in_out_puts('SNF-0203-NEWYORKf-09dlc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also available as a command line\n",
    "\n",
    "        SnfFclass -j SNF-0203-NEWYORKf-09dlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The SNfactory database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNfactory database (DB) is a ProstgreSQL database (ver. 9.3.5) interfaced with the python Django framework (ver. (0, 97, 'pre')). It has a current size of 15.4GB (as of Wed. Dec. 2, 2015) and is hosted on a shared server (*ccpgsql.in2p2.fr*, on port 5432) currently containing 33 other DBs. No size limitation is given to a specific DB, but a total of 500GB is avalaible on this server, of which about 100GB is currently used. A complete backup of the server and of the transaction logs are made every day at 8pm (French time), and are kept 180 days. This allows a complete restoration of the server at any time over the 6 past months. In case of accident/incident on the SNfactory DB, a request for restoration through the [CC-IN2P3 user ticket](https://cc-usersupport.in2p3.fr/otrs/customer.pl) can be done. Since this type of intervention is quite heavy, it is better to inform the CC of any operation that could impact the DB to ask for a backup beforehand. On this DB, the maximum number of simultaneous connections is 400, which allows us to run up to 400 jobs at the same time, the current limit being actually 360 (see below in the section about the production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNfactory database contains 7 major *tables* (or *models* in the Django vocabulary) containing all the information about our targets, runs, processes, jobs, etc. They are shown in the following figure, and are, in order: \n",
    "    \n",
    "* **Target**: Obvious target information (name, type, coordinates, etc.). A Target entry has-many Runs.\n",
    "* **Run**: Description of the target pointing, corresponding to an event at the summit (date, pointing type, etc.). A Run entry has-many Exposures;\n",
    "* **Exposure**: Description of the different data taking condition of a pointing (data and condition of the acquisition, science or calibration pose, P/B/R channels or a combination of the three, etc.). An Exposure entry has-many Poses;\n",
    "* **Pose**: Decritpion of the CCD poses coming from the acquisition (exposure time, guiding and pose qualities, etc.). A Pose entry has-many Processes;\n",
    "* **Process**: The basic semantic unit in the pipeline. A Process is an instance of some operation product. It is identified by a labeling system called \"[X]Fclass\". A Process has many Files. Examples:\n",
    "    * A raw-data file is a process.\n",
    "    * Preprocessed (overscan+bias+dark) data file is a process.\n",
    "    * An extracted spectrum, before or after flux calibration, is a process (both can be processes in the database).        \n",
    "* **File**: Description of the files produced by the data processing codes (name, size, checksum, etc.). May be raw-data or processed data.;\n",
    "* **Job**: Description of the agents having supervised the processing operations in the worker farm at CC-IN2P3 run by the pipeline (name, version, state, date, Qsub command used, etc.)\n",
    "      \n",
    "These tables are linked together according to the scheme shown below, which also gives most of their attributes. Each table has a primary key (id) which is what foreign keys in other tables point to (the IdXXX columns). There is a secondary sort of key with a name like IdTarget or IdProcess, more often used as a handle in your programs. These models are available in the SNfactory processing package and can be imported as followed:\n",
    "\n",
    "    from processing.process.models import Target, Run, Exposure, Pose, Process, File, Job\n",
    "\n",
    "The database code is avaible on the [SNfactory CVS](https://cvs.in2p3.fr/snovae-SNFactory/), and is located under\n",
    "\n",
    "    SNFactory/Tasks/Processing/database/django/processing_095]\n",
    "\n",
    "It is used in almost all the pipeline steps in order to query or to save data. If you have it installed on your personnal computer and are not in a IN2P3 network, you can still have a local access to the DB by tunneling localhost:5432 to ccpgsql.in2p3.fr:5432, e.g.:\n",
    "\n",
    "ssh -C -N snprod@ccage.in2p3.fr -L 5432/ccpgsql.in2p3.fr/5432\n",
    "\n",
    "For a more complete description of the DB tables and their content, please have a look at [this document](https://snf-doc.lbl.gov/twiki/pub/Offline/DataAccessOverview/snf_process.pdf), section 4.2.2 (\"Tables description\"), page 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Snovae DB](figures/SnovaeDB3.png)](figures/SnovaeDB3.png \"SNfactory DB scheme\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
