{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code description\n",
    "\n",
    "Here is the list of all codes used from the data transfert to the IDR and Zoo, a short description of what they do, some examples or documentation, main dependencies, and comments. Almost all these steps will have an interaction to the SNfactory DB, which is presented in the [general overview](SNfactory_DB_Overview.html#the-snfactory-database). This section is intented to give the main purpose of all the pipeline steps. For a full description on how to run the pipeline, please refer to the data processing [things to know](DPThingsToKnow.html) and [cookbook](SNfactoryDataProcessing.html) sections. \n",
    "\n",
    "For each code/step, the description will have the following structure:\n",
    "\n",
    "* **Purpose**: The main goal of the script/step.\n",
    "* **Who**: Who should run it.\n",
    "* **When**: When to run it.\n",
    "* **Where**: Where to run it.\n",
    "* **What**: What does it do/run/produce.\n",
    "* **On**: What to run it on (target, night, etc.).\n",
    "* **DB**: Does it need a DB interaction (YES or NO).\n",
    "* **Dependencies**: List of the main non-standard dependencies.\n",
    "* **Inputs**: Any needed inputs.\n",
    "* **Outputs**: Description of the outputs.\n",
    "* **Comments**: Any other useful comments on the script/step.\n",
    "* **CVS link**: Pointer to the online CVS link.\n",
    "\n",
    "All the steps done at the CC-IN2P3 have to be run under the **snprod account**. Ask the current data-production team or find someone in the collaboration to get the password of this account.\n",
    "\n",
    "**Notes on the dependency graphs** [g]: \n",
    "\n",
    "* Only non-standard Python libraries have been included. \n",
    "* Django has been reduced into a single bubble. \n",
    "* Automatically ignore unused imports and follow the modules depended upon and trace their dependencies.\n",
    "* Produced using `Snakefood` ([see here](http://furius.ca/snakefood/doc/snakefood-doc.html)) and `dot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transfer and summit cleaning\n",
    "\n",
    "### `export_sync`\n",
    "\n",
    "* **Purpose**: Makes sure all data are sent from the summit to the CC IN2P3.\n",
    "* **Who**: Automatic process.\n",
    "* **When**: Every few days, or after a set of 4-6 nights of observations at most.\n",
    "* **Where**: Summit computer.\n",
    "* **What**: Compare the content of the CC disk with what is on disk at the summit.\n",
    "* **On**: All files of a night.\n",
    "* **DB**: NO.\n",
    "* **Dependencies**:\n",
    "    * Uses `File::Basename`\n",
    "    * Requires `getopts.pl`\n",
    "* **Inputs**: None.\n",
    "* **Outputs**: None.\n",
    "* **Comments**: \n",
    "    * The person responsible for the data transfer usually makes sure that all the data are indeed transfered before cleaning the summit.\n",
    "    * `Perl` script.\n",
    "* **CVS link**: [export_sync.pl](https://cvs.in2p3.fr/snovae-SNFactory/Online/register/export_sync.pl)\n",
    "\n",
    "    \n",
    "### `hsi_import`\n",
    "    \n",
    "* **Purpose**: Copy the new data to the HPSS disk at NERSC.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: Every few days, or after a set of 4-6 nights of observations at most (after `export_sync`).\n",
    "* **Where**: Under /afs/in2p3.fr/home/s/snprod/data_copy at CC IN2P3.\n",
    "* **What**: Copy the files taken during a set of nights from the CC-IN2P3 to the HPSS (NERSC).\n",
    "* **On**: A selection of nights.\n",
    "* **DB**: NO.\n",
    "* **Dependencies**:\n",
    "    * Use Getopt::Std,\n",
    "    * Use File::Basename.\n",
    "* **Inputs**: All files of a night on the CC disks.\n",
    "* **Outputs**: The exact same files on the HPSS disks at NERSC.\n",
    "* **Comments**: \n",
    "    * This is a file by file copy, which is not the best for an easy access later on for HPSS disks.\n",
    "    * `Perl` script.\n",
    "* **CVS link**: [hsi_import.pl](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/hsi_import.pl)\n",
    "\n",
    "\n",
    "### `\"summit cleaning\"`\n",
    "\n",
    "* **Purpose**: Clean the summit disks to leave anough space for new observation.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: After `hsi_import` and manual check of the copies.\n",
    "* **Where**: On the summit disks (/lbl2/data/ and /lbl3/data/).\n",
    "* **What**: Delete from the summit disks all the files already transfered to the CC and NERSC disks.\n",
    "* **On**: All nights already transfered to the CC.\n",
    "* **DB**: NO.\n",
    "* **Dependencies**: None.\n",
    "* **Inputs**: None.\n",
    "* **Outputs**: None.\n",
    "* **Comments**: \n",
    "    * Make sure that all copies were correctly done before deleting anything.\n",
    "* **CVS link**: None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB header update and data filling\n",
    "\n",
    "### `snf_header`\n",
    "\n",
    "* **Purpose**: Update the DB header with the new informations.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: Usually right after the data transfer and summit cleaning steps.\n",
    "* **Where**: Under /afs/in2p3.fr/home/s/snprod/db/SGE at CC IN2P3.\n",
    "* **What**: Run `SnfDjangoMigrUpdate -u YEAR -u DAY -v` where \"YEAR\" is the current year, \"DAY\" is the last day to update, and \"v\" is for use of the verbose mode.\n",
    "    * Get the name of the last file included in the Header DB table\n",
    "    * Create a new range of days from this last day up to the given day (+1)\n",
    "    * Fetch the data in /sps/snovae/SRBregister/hawaii\n",
    "    * Loop over each new day to include in the DB. For each day, get the list of file (`gen_list.Walk`). Look for specific pattern: (`\"*vid.fits;*acq*fits;*/??_???_???_???_??_?.fits\"`)\n",
    "    * Clean and order the obtained list of nights (SnfDjangoMigr2.clean_double_list)\n",
    "    * Make sure they do not already exist in the DB\n",
    "    * Migrate the headers data from the fits file to the DB\n",
    "        * Get the list and clean it again\n",
    "        * Connect to the DB\n",
    "        * Iterate on each group of fits files (per/day)\n",
    "        * For a given night, read all headers using the `listhead` script (`cfitio`, under /afs/in2p3.fr/home/throng/snovae/snf/SNFactory/Online/cfitsio) in the `read_fits_files` function\n",
    "        * Prepare the SQL command to create/modify the table\n",
    "        * Parse the variable values and create finaly create the headers in django\n",
    "* **On**: A set of nights.\n",
    "* **DB**: YES.\n",
    "* **Dependencies**:\n",
    "    * `SnfDjangoMigrUpdate` [[g](figures/SnfDjangoMigrUpdate.pdf)] (under SNFactory/Tasks/Processing/database/head_import)\n",
    "        * DB models: `from processing.process.models import Header`\n",
    "        * `SnfDjangoMigr2` [[g](figures/SnfDjangoMigr2.pdf)]\n",
    "            * `django.db`\n",
    "            * `processing.settings`\n",
    "            * `psycopg`\n",
    "        * `gen_list` [[g](figures/gen_list.pdf)]:\n",
    "            * `clean_double_list` from `SnfDjangoMigr2`\n",
    "* **Inputs**: A set of night for which the fits file will be parse from /sps/snovae/RBregister/hawaii\n",
    "* **Outputs**: No output. Fill the Header DB table.   \n",
    "* **Comments**: \n",
    "    * Shell script (csh) running different python scripts.\n",
    "    * There isn't any modification of the fits file during this step!\n",
    "    * Still unclear: Where are the header values exactly stored in the DB?\n",
    "* **CVS link**: None.\n",
    "\n",
    "### `snf_db_make`\n",
    "\n",
    "* **Purpose**: Create a pickle file (actually a `shelve` file) containg all the new data info.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: Right after `snf_header`\n",
    "* **Where**: Under /afs/in2p3.fr/home/s/snprod/db/SGE at CC IN2P3.\n",
    "* **What**: Create a pickle file later used to fill the DB\n",
    "    * Create a new pick file in /afs/in2p3.fr/group/snovae/snprod/db\\_pickle/\n",
    "    * Run `Meteo -a`:\n",
    "        * -a: fetch only missing CFHT files from hawaii\n",
    "        * if year < 2008, fetch the meteo files from http://mkwc.ifa.hawaii.edu/archive/wx/cfht\n",
    "        * else, get them from `sne@kole.cfht.hawaii.edu` using ssh (`ssh cfht \"df /data/logger\"`)\n",
    "        * store them under `/sps/snovae/SRBregister/log/cfht`:\n",
    "            * in their initial format (.dat file)\n",
    "            * update (or create) a `cfht-wx.YYYY.pdb` file containing meteo info for all days of a year using Meteo.MeteoFile. This class read the CFHT meteo log, write and read the output .pdb file, and can also extrapolate meteo for missing day.\n",
    "    * Run `FetchIAUC`:\n",
    "        * Fetch the IAUC targets from http://www.cbat.eps.harvard.edu/lists/Supernovae.html\n",
    "        * Update `/afs/in2p3.fr/home/throng/snovae/doc/SNFactory/online/logbook/sn_iauc.txt`\n",
    "            * Uses SnfTarget.BuildIaucTargetFile('sn_iauc.txt')\n",
    "            * Realized that \"LOSS\" targets became \"Lick Observatory Supernova Survey\" since April 2013. Since this date, the script actually (quietly) crashed and was not updating the file due to differences between the local file and the online data.\n",
    "    * Creating the DB pickle file: `SnfUpdate -y YEAR -f FROMDAY -t TO_DAY -d PICKLEFILE`:\n",
    "        * -y: Data taking year (usually the current year)\n",
    "        * -f: Data taking day to start the DB filling within the year \n",
    "        * -t: Data taking day to stop the DB filling within the year\n",
    "        * -d: Python File to produce or load in DB (-l option, used in snf_db_fill)\n",
    "        * Update the SNf processing data base with raw data. This script reads all the available sources to generate the entries for the processing database for a given set of nights. \n",
    "        * It actually builds a string with all the snifs_run_YY_DDD files found under /sps/snovae/SRBregister/log for the corresponding nights, and then cat all of them into the file snifs_run_all, which is then processed by SnfRun.SnfData.\n",
    "        * In the process, all the logs stored in this directory are read by SnfParseLog.FnameLog, called by SnfRun.SnfPose, itself called by SnfRead, in SnfData.\n",
    "        * As an example, the `zelog` file will be opened and the Pose DB table will be updated using its content.\n",
    "            * It uses SnfParseLog.SnfzeLog and SnfParseLog.SnfzeLogLine.\n",
    "            * If there is not info, Qzelog is set to 0, if we have info, it is set to 1. It can also be set to 2 in case of warning, or 3 in case of error. This value is then used in most of the queries made in the pipeline.\n",
    "        * These logs have been copied from the summit by log_export. All of them have been built at the summit during or after observation. As an example, the SkyProbe log is created by `SNFactory/Online/Acq_scripts/yesterday_archive`, which perform all the archive/check associated to the last data taking day (UTC), as well as download at summit skyprobe data before log export.\n",
    "* **On**: \n",
    "* **DB**: YES.\n",
    "* **Dependencies**:\n",
    "    * `Meteo.py` [[g](figures/Meteo.pdf)] (under SNFactory/Tasks/Processing/database/SnfObj)\n",
    "        * `SnfMonitor.py` [[g](figures/SnfMonitor.pdf)]\n",
    "        * `JdUtc.py` [[g](figures/JdUtc.pdf)]\n",
    "    * `FetchIAUC.py` [[g](figures/FetchIAUC.pdf)]\n",
    "        * `SnfTarget.py` [[g](figures/SnfTarget.pdf)]\n",
    "            * `RaDec.py` [[g](figures/RaDec.pdf)]\n",
    "    * `SnfUpdate.py` [[g](figures/SnfUpdate.pdf)]\n",
    "* **Inputs**: A set of night, for which the log files will be automatically fetched (from /sps/snovae/SRBregister/log)\n",
    "* **Outputs**: A single pickle file.   \n",
    "* **Comments**: \n",
    "    * Shell script (csh) running several Python scripts.\n",
    "    * The CFHT meteo files and a current IAUC list are fetched automatically during the execution of snf_db_make.\n",
    "    * The output file is actually a shelve file, opened with the `shelve` python library (pickle lobrary won't work).\n",
    "* **CVS link**: None.\n",
    "\n",
    "### `snf_db_fill`\n",
    "\n",
    "* **Purpose**: Fill the DB using the previoulsy made pickle file.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: Right after `snf_db_make`\n",
    "* **Where**: Under /afs/in2p3.fr/home/s/snprod/db/SGE at CC IN2P3.\n",
    "* **What**: Use the latest pickle created to fill the DB.\n",
    "* **On**: A pickel file generated by `snf_db_make`\n",
    "* **DB**: YES.\n",
    "* **Dependencies**:\n",
    "    * `SnfUpdate.py` [[g](figures/SnfUpdate.pdf)]\n",
    "    * `SnfFillOn.py` [[g](figures/SnfFillOn.pdf)]\n",
    "    * `parse_runs.py` [[g](figures/parse_runs.pdf)] (under /afs/in2p3.fr/group/snovae/snprod1/webserver_data/)\n",
    "        * This script will update a pickle file used to list on nights with observtions on the [agenda webpage](snf.in2p3.fr/agenda).\n",
    "* **Inputs**: A single pickle file.\n",
    "* **Outputs**: No output. Fill the DB.   \n",
    "* **Comments**: \n",
    "    * Shell script (csh) running several Python scripts.   \n",
    "    * `snf_db_fill` does not need to be modified: it will search for the latest created pickle file and insert its contents into the DB.\n",
    "    * The [night plots](http://snovae.in2p3.fr/snprod/PhotoNight/) and SkyProbe photometricity information are fetched automatically.\n",
    "* **CVS link**: None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB update\n",
    "\n",
    "### `SyncTarget.py`\n",
    "\n",
    "* **Purpose**: Synchronize the new target information found in WareHouse with the DB information\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: Can be done anytime, but is usually done after the header update and data filling steps.\n",
    "* **Where**: From anywhere at CC-IN2P3.\n",
    "* **What**: Update the database with warehouse information (z, Ebmv).\n",
    "* **On**: Fetches local files, DB, Warehouse, IRSA or NED.\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/SyncTarget.pdf)]:\n",
    "    * `Django` + DB codes\n",
    "    * `SnfUtil.query_warehouse(q)` (q is a SQL query)\n",
    "    * `SnfTarget.ReadIaucTarget('sn_iauc.txt', [])`\n",
    "    * `from ToolBox.Astro import Fetchers`:\n",
    "        * `Fetchers.sfd_ebmv` (fetch SFD Milky Way E(B-V) from IRSA (http://irsa.ipac.caltech.edu/)\n",
    "* **Inputs**: Different input from the web (IRSA), from warehouse, and from the IAUC local file.\n",
    "* **Outputs**: Update of DB tables\n",
    "* **Comments**: \n",
    "    * Default options used are -R (--redshift) and -E (--extinction)\n",
    "* **CVS link**: [SyncTarget.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/database/SnfObj/SyncTarget.py)\n",
    "\n",
    "### `FlagRunKind.py`\n",
    "\n",
    "* **Purpose**:  Check the Run.Kind for errors or mismatches and update it if needed, e.g., to define if a spectrum is a final reference (according to its time difference with the previous observation), or to check for new SCALA data.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: Can be done anytime, but is usually done right after SyncTarget.\n",
    "* **Where**: From anywhere at CC-IN2P3.\n",
    "* **What**: Flag Run.Kind for three types of \"targets\"\n",
    "    * SuperNova (`process_SNe`)\n",
    "        * Work on all targets and runs compliant with:\n",
    "            * Target.Kind='SuperNova'\n",
    "            * Target.Type[0:2] in ['Ia', 'II'] with at least one run\n",
    "            * All other supernova ('Ic', 'Ib', 'Ib/c') with at least 5 runs\n",
    "            * Run.Type='SPECTRED'\n",
    "        * Loop on all the runs of each target, and check their Kind, which should be in the following 'official' list of kinds:\n",
    "            * Possible Kinds:\n",
    "                * 'HostGalaxy': Host spectrum, centered on the core of the galaxy,\n",
    "                * 'RefGalaxy': Not used,\n",
    "                * 'FinalRef': Host spectrum, centered on the SN location,\n",
    "                * 'PhotoFinalRef': Host photometry, centered on the SN location,\n",
    "                * 'FollowUp': Supernova follow-up spectrum,\n",
    "                * 'PhotoFollowUp': Supernova follow-up photometry,\n",
    "                * 'ScreenR': Unfollowed SN spectrum (< 3 epochs, but with a spectro reference),\n",
    "                * 'PhotoScreenR': Unfollowed SN photometry (< 3 epochs, but with a spectro reference).\n",
    "            * If not in the abose list:\n",
    "                * If there is no Fclass=17 exposure, do nothing\n",
    "                * Else, check the delay time between the current run and the previous one, and flag it in consequence:\n",
    "                    * delay > 250 days, or the run is the one of an 'H-alpha' host, flag the Run.Kind as:\n",
    "                        * 'FinalRef' if Run.Type is 'SPECTRED'\n",
    "                        * 'PhotoFinalRef' if Run.Type is 'ACQREF' (or actually anything else...)\n",
    "                    * delay <= 250 days:\n",
    "                        * If the total number or run exceed 2:\n",
    "                            * 'FollowUp' if Run.Type is 'SPECTRED'\n",
    "                            * 'PhotoFollowUp' otherwise\n",
    "                        * If the total number or run is less than 2:\n",
    "                            * 'ScreenR' if 'SPECTRED'\n",
    "                            * 'PhotoScreenR' otherwise\n",
    "    * StdStar (`process_Std`)\n",
    "        * For all standard stars, check all SPECTRED runs and their associated pose exposure-time\n",
    "        * For a given target (having at least 5 runs), compute the median exposure time\n",
    "        * Flag each run Kind to 'Misc' if its exposure time is inferior to 25% of the median value\n",
    "    * Scala runs (`process_Scala`)\n",
    "        * Get all the SCALA raw data (Fclass=60, Version=0)\n",
    "        * Change all the corresponding Run.Kind and Run.Type to 'Calib' and 'SCALA', respectively\n",
    "        * Also check for empty exposure, and delete them if any. By default, 100 exposures are created for a SCALA run, which are not always used since a SCALA run has a variable number of exposures.\n",
    "* **On**: The Run DB table .\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/FlagRunKind.pdf)]:\n",
    "    * `Django` + DB codes\n",
    "    * `SnfMonitor.Monitor`\n",
    "    * `JdUtc.JdSub`\n",
    "* **Inputs**: DB information\n",
    "* **Outputs**: Modification in the Run DB table.\n",
    "* **Comments**: \n",
    "    * The default options, currently used in the data processing, do not includes the Standard stars.\n",
    "    * In the current code, Ib/c targets with less than 5 runs will be forgotten.\n",
    "* **CVS link**: [FlagRunKind.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/database/SnfObj/FlagRunKind.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "All the following scripts are usually run with the `snprod` account at CC-IN2P3 under \n",
    "\n",
    "        /afs/in2p3.fr/group/snovae/snprodJob/VERSION\n",
    "        \n",
    "where `VERSION` is the code version in use (e.g., SNF-02-03). You can also acces this directory with\n",
    "\n",
    "        cd $JOBDIR\n",
    "\n",
    "For a more detailed overview of some parts of the pipeline, you can have a look at the [cube reduction flow](figures/CubeReductionFlow.pdf) and [flux calibration flow](figures/CalibrationFlowMultiNP.pdf) figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_synthetic_arcs`\n",
    "\n",
    "* **Purpose**:\n",
    "    * Get the synthetic arcs from `/sps/snovae/user/bhayden/arcs` and register them into the DB.\n",
    "    * Only new arcs (i.e, not in the DB) will be registered.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: When new synthetic arcs are produced and placed under `/sps/snovae/user/bhayden/arcs`.\n",
    "* **Where**:\n",
    "    * Using the `snprod` account at CC-IN2P3.\n",
    "    * Under `/afs/in2p3.fr/group/snovae/snprodJob/VERSION/PSA`.\n",
    "* **What**: \n",
    "    * First make sure the arcs directory does exist\n",
    "    * Create a new job name using the list of already existing PSA jobs found in the DB (incrementnal process)\n",
    "    * Get the list of new synthetic arcs to add to the DB\n",
    "        * Get the list of synthetic arcs stored in the arcs directory\n",
    "        * Get the list of synthetic arcs stored in the DB\n",
    "        * Compare the two list and prepare to add newly created synthetic arcs\n",
    "        * If more than 200 arcs, split the work into several jobs    \n",
    "    * Loop on the list of arcs and register them\n",
    "        * First copy them on the worker.\n",
    "        * Then register them in the DB with a Fclass of 4 (arc) and an XFclass of 901 (synthetic).\n",
    "        * For consistency purpose, we want the synthetic arcs to have for parents their corresponding raw data fits files (017_000).\n",
    "* **On**: A list of synthetic arcs produced and stored under `/sps/snovae/user/bhayden/arcs` at CC-IN2P3.\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/plan_synthetic_arcs.pdf)]:\n",
    "    * Django + DB codes + SNfObj library\n",
    "* **Inputs**: The following inputs aren't actually used anywhere in the plan. They are being used for parenting purpose only.\n",
    "    * 17 / 0: Raw object frame\n",
    "* **Outputs**: The synthetic arcs aren't actually produced in the plan, but only fetched from an already existing directory, copied into `sps` and finally included into the DB.\n",
    "    * 4 / 901: Synthetic arc frame\n",
    "* **Comments**:\n",
    "    * Here are examples of [.sh](http://snf.in2p3.fr/file/25010590/content/), [.err](http://snf.in2p3.fr/file/25011959/content/) and [.out](http://snf.in2p3.fr/file/25011958/content/) files created for a [typical PFS job](http://snf.in2p3.fr/job/193279/).\n",
    "    * There is actually no real input/output in this plan. All the computing part are done by a code made by [Brian Hayden](mailto:bhayden@lbl.gov). \n",
    "    * Comment from Brian: \n",
    "        \n",
    "            The plan is to work with Kyle Barbary eventually to get it integrated into the actual SNF code. Currently, all of the code is on NERSC. When I run it I have to have every pre-processed arc on disk at NERSC. To generate arcs it requires some version of the science image so that it can pull stuff out of the header. The very poor documentation with reminders for myself for how to run the code is currently on NERSC at: /project/projectdirs/snfactry/users/bhayden/arcs_update1314/README.\n",
    "        \n",
    "* **CVS link**: [plan_synthetic_arcs.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_synthetic_arcs.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_file_quality`\n",
    "\n",
    "* **Purpose**: Generate script to subsequently execute simple preprocessing pipeline with quality check possibility.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: After a few nights of observation, when the previous steps are done, or for a full re-processing of the data.\n",
    "* **Where**: \n",
    "    * Using the `snprod` account at CC-IN2P3\n",
    "    * Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/PFQ        \n",
    "* **What**: This plan file runs the preprocessing and the update of the DB quality flag from a list of nights file (format YY_DDD, 1 per line). By default it will: \n",
    "    * Copy the raw file on the worker, selected by\n",
    "        * Pose_FK__Qzelog <= 3,                    \n",
    "        * Status = 1,\n",
    "        * Has an associated file\n",
    "        * Plus selection on the channel and specific processes exclusion if asked\n",
    "    * Fits header update\n",
    "        * `from plan_file_preprocessing import FitsUpdateCommand`\n",
    "        * Using script `FitsUpdate`, under `SNFactory/Tasks/Processing/database/SnfObj`\n",
    "        * Will update the fits header using DB information like:\n",
    "            * EFFTIME: Exposure time including shutter overhead,\n",
    "            * JD: JD at the midle of the exposure,\n",
    "            * AIRMASS, PARANG: AIRMASS and PARalactic ANGle at the midle of the exposure,          \n",
    "            * etc. (about 25 parameters updated/added)\n",
    "        * New info are first stored in a `FitsUpdate.data`. The *fits* file is then updated with `fmodhead`:\n",
    "            * `/usr/local/heasoft/heasoft-6.14/x86_64-unknown-linux-gnu-libc2.12/bin/fmodhead`     \n",
    "        * `fmodhead` has apparently been introduced for practical reasons in `FitsUpdate.py` because of some errors appearing with `pyfits` while doing some operations.\n",
    "        * An intermediate file (afs/in2p3.fr/home/s/snprod/pfiles/fmodhead.par) is localy copied on the worker when `fmodhead` is used in order to prevent parralel batch jobs problems (see [here](http://heasarc.gsfc.nasa.gov/docs/software/lheasoft/scripting.html)). For this purpose, the PFILES enrironment variable is thus modified in PFQ while creating the `.sh` script.\n",
    "    * Preprocessing\n",
    "        * Script used: `preprocess.cxx`, under `IFU/Ccd/pkg/preprocess/source/`\n",
    "        * With or without dark and bias modeling function of the file/channel:\n",
    "            * No bias and no dark (simple preprocessing) if:\n",
    "                * P channel, raster, raw bias frame (Fclass=24), or no OPENTIME information\n",
    "            * Bias but no dark if raw dark frame (Fclass=25)\n",
    "            * Bias and dark for other files (B and R channels)\n",
    "        * Bias and dark models copied localy (on the worker) from IFU/Ccd/user/data/\n",
    "            * biasmodel\\_\\*.txt\n",
    "            * bias\\_\\*.fits\n",
    "            * darkmodel\\_\\*.txt\n",
    "            * dark\\_\\*.fits\n",
    "    * Compute the quality flags and update them in the DB\n",
    "        * Implements command to do a quality check of a catalog of  FITS file\n",
    "        * Quality cuts are stored under SNFactory/TASKS/data/qualitycuts.txt\n",
    "        * Script used: `quick_anal`, under `IFU/Ccd/user/bin/quick_anal`\n",
    "        * Input is the preprocessed file, output is the `.zelog` file\n",
    "    * Save the preprocess and quality files in the DB\n",
    "        * `UpdateZelogDB`, code under SNFactory/TASKS/database/SnfObj/UpdateZelogDB\n",
    "            * Update the DB Pose table using the `.zelog` file content\n",
    "            * Some of the updated values: Mean, RMS, NUnder, Satu, Q99, Q999, Qzelog, etc.\n",
    "        * `record_timestamp` to save time info in the yaml log file\n",
    "        * `accountant` to register everything into the DB using the yaml file content\n",
    "    * In case of need for: \n",
    "        * re-preprocessing, and if the quality flag does not need to be rerun, use the option *--no_quality*\n",
    "        * re-run the quality flag DB update, but do not save any file, use the option *--donot_save_file*\n",
    "* **On**: A set of nights.\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/plan_file_quality.pdf)]:\n",
    "    * `SnfCommand.Command`\n",
    "    * `SnfWrite.splitfname`\n",
    "    * `plan_file_preprocessing.FitsUpdateCommand`\n",
    "        * FitsUpdate [[g](figures/FitsUpdate.pdf)]\n",
    "            * `fmodhead`\n",
    "            * Different Snf libraries:\n",
    "                * `JdUtc.JdUtc`\n",
    "                * `RaDec.RaDec`\n",
    "                * `SnfDjangoLoadDB.LoadPartDB`\n",
    "                * `SnfRun.SnfDataRead`\n",
    "                * `SnfWrite.SnfFName and SnfWrite.SnfChannel\n",
    "* **Inputs**: Raw data frames\n",
    "    * 3 / 0: Raw arc frame\n",
    "    * 7 / 0: Raw continuum frame\n",
    "    * 12 / 0: Raw twilight frame\n",
    "    * 17 / 0: Raw object frame\n",
    "    * 52 / 0: Raw dome frame\n",
    "    * 60 / 0: Raw SCALA frame\n",
    "* **Outputs**: Preprocessed data frames (with their updated header) and the associated quality file (`.zelog`) from the quality analysis:\n",
    "    * 4 / 0: Preprocessed arc frame\n",
    "    * 4 / 207: Preprocessed arc frame with bias and dark\n",
    "    * 8 / 0: Preprocessed continuum frame\n",
    "    * 8 / 207: Preprocessed continuum frame with bias and dark\n",
    "    * 13 / 0: Preprocessed twilight frame\n",
    "    * 13 / 207: Preprocessed twilight frame with bias and dark\n",
    "    * 18 / 0: Preprocessed object frame\n",
    "    * 18 / 207: Preprocessed object frame with bias and dark\n",
    "    * 54 / 0: Preprocessed dome frame\n",
    "    * 54 / 207: Preprocessed dome frame with bias and dark\n",
    "    * 61 / 0: Preprocessed SCALA frame\n",
    "    * 61 / 207: Preprocessed SCALA frame with bias and dark\n",
    "    * 70 / 0: Preprocessed dark frame\n",
    "* **Comments**: \n",
    "    * Here are examples of [.sh](http://snf.in2p3.fr/file/31937558/content/), [.err](http://snf.in2p3.fr/file/31943381/content/) and [.out](http://snf.in2p3.fr/file/31943378/content/) files created for a [typical PFS job](http://snf.in2p3.fr/job/245388/).\n",
    "    * The files are locally copied, then their header is updated. They are then used in the preprocessing steps and deleted. The initial files are thus unmodifed, while the preprocessed ones have their header updated, since coming from the updated version of the orginal ones.\n",
    "* **CVS link**: [plan_file_quality.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_file_quality.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_cube_generation`\n",
    "\n",
    "* **Purpose**: Generate script to subsequently generate cubes.\n",
    "* **Who**: Person in charge of the production.\n",
    "* **When**: After `plan_file_quality`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/PCG\n",
    "* **What**:\n",
    "    * Select and copy all the needed files\n",
    "        * Run type must be in [SPECTRED, DOME, SKYFLAT, SCALA]\n",
    "        * Target must have:\n",
    "            * PI = SNFactory\n",
    "            * Kind in [StdStard, SuperNova]\n",
    "        * File types are listed below in the \"**Inputs**\" bullet\n",
    "    * Run `fit_background`\n",
    "        * Use continuum frames to select pixels for background fit\n",
    "        * First, get the dark map from IFU/Ccd/user/data/dark\\_*.fits\n",
    "        * And then apply `fit_background` on\n",
    "            * Continua, and save background pixel map\n",
    "                * For fclass = 8 (options -b outtab,c -d THEDARKMAP)\n",
    "            * On data using the background pixel map previously estimated\n",
    "                * For fclass in [54, 18] (options -b intab -d THEDARKMAP)\n",
    "    * Run `quick_pipeline.py`\n",
    "        * Serie of command lines containing the scripts listed in the \"**Dependencies**\" bullet.\n",
    "        * It will be run on:\n",
    "            * flat field poses (`quick_cube` and `comp_lfff`)\n",
    "            * calibration poses (`quick_cube`, `quick_calib`, `convert_file`)\n",
    "            * science poses (all scripts): \n",
    "                * `quick_cube`: Calibration/extraction of a cube using a calibration arc and a continuum raster frame.\n",
    "                * `adjust_dichroic`: Remove for the dichroic features (on B and R).\n",
    "                * `comp_lfff`: Flat-field computation on the dichroic corrected cubes (B and R).\n",
    "                * `quick_calib`: Apply the flat field.\n",
    "                * `convert_file`: Convert the .tig cube into an euro3d format.\n",
    "                * `quick_extract`: A quick (and dirty) extraction of the target.\n",
    "                * `fits_to_yaml`: Save HEADER info (position of the target) into the main yaml log.\n",
    "                * `quick_plot`: A plot of the extracted spectrum.\n",
    "        * The content of the produced `quick_pipeline` `.sh` script will then be inserted into the main `.sh` script, which will be the one submited to the worker.\n",
    "* **On**: A set of nights.\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/plan_cube_generation.pdf)]\n",
    "    * `fit_background.py`\n",
    "        * Purpose: Ad-hoc background fit and subtraction.\n",
    "        * Under IFU/Snifs/pkg/pipeline/background/\n",
    "    * `quick_pipeline.py` (under IFU/Snifs/pkg/pipeline/tools/)\n",
    "        * `quick_cube.sh`\n",
    "            * Purpose: Quick preprocessing, datacube extraction and wavelength calibration.\n",
    "            * Under IFU/Snifs/pkg/pipeline/quick/\n",
    "            * Runs (case dependent)\n",
    "                * `preprocess.cxx`\n",
    "                    * if needed\n",
    "                    * under IFU/Ccd/pkg/preprocess/source/\n",
    "                * `embed_raster.py` \n",
    "                    * if a frame is a raster\n",
    "                    * under /IFU/Snifs/pkg/pipeline/tools/\n",
    "                * `wr_desc.c` \n",
    "                    * keywords copy from one channel to the other\n",
    "                    * under IFU/IFU_gentools-6.4/pkg/tools/source/\n",
    "                * `extract_spec2.c`\n",
    "                    * spectra extraction (CCD to datacube)\n",
    "                    * under IFU/Snifs/pkg/extract/source/ \n",
    "                * `quick_ima.sh`\n",
    "                    * quick (I,J)-image reconstruction\n",
    "                    * under IFU/Snifs/pkg/pipeline/quick/\n",
    "                * `center_gauss.c` \n",
    "                    * center a 2D-gaussian on input-frame\n",
    "                    * under IFU/Snifs/pkg/focus/source/ \n",
    "                * `wcalib.c` \n",
    "                    * wavelength calibration computation\n",
    "                    * under IFU/Snifs/pkg/calib/source/ \n",
    "                * `wrebin.c`\n",
    "                    * wavelength calibration application\n",
    "                    * under IFU/Snifs/pkg/calib/source/ \n",
    "        * `adjust_dichroic.py`\n",
    "            * Purpose: Remove the dichroic features of the input cubes.\n",
    "            * Under IFU/Snifs/pkg/pipeline/dichroic/\n",
    "        * `comp_lfff.c` \n",
    "            * Purpose: Low-frequency spectro-spatial flat-field (computation)\n",
    "            * Under IFU/Snifs/pkg/calib/source/\n",
    "        * `convert_file.c` \n",
    "            * Purpose: Interface to Convert File from different data format\n",
    "            * Under IFU/IFU_gentools-6.4/pkg/tools/source/\n",
    "        * `quick_calib.sh`\n",
    "            * Purpose: Quick datacube calibration (LFFF, cosmics, flux).\n",
    "            * Under IFU/Snifs/pkg/pipeline/quick/\n",
    "            * Runs\n",
    "                * `apply_lfff.c`\n",
    "                    * spectro-spatial flatfield\n",
    "                    * under IFU/Snifs/pkg/calib/source/ \n",
    "                * `remcosmic.c`\n",
    "                    * cosmic cleaning\n",
    "                    * under IFU/Snifs/pkg/calib/source/\n",
    "                * `apply_flux.c`\n",
    "                    * flux calibration\n",
    "                    * under IFU/Snifs/pkg/calib/source/\n",
    "                * `truncate_cube.c`\n",
    "                    * cube truncation\n",
    "                    * under IFU/Snifs/pkg/calib/source/\n",
    "                * `quick_ima.sh`\n",
    "                    * image reconstrution\n",
    "                    * under IFU/Snifs/pkg/pipeline/quick/ \n",
    "        * `quick_extract.sh`\n",
    "            * Purpose: Quick supernova/star spectrum extraction.\n",
    "            * Under IFU/Snifs/pkg/pipeline/quick/\n",
    "            * Runs\n",
    "                * `quick_ima.sh`\n",
    "                    * image reconstrution\n",
    "                    * under IFU/Snifs/pkg/pipeline/quick/\n",
    "                * `info_tiger.c`\n",
    "                    * get name of the table associated to the cube\n",
    "                    * under IFU/IFU_gentools-6.4/pkg/tools/source/\n",
    "                * `center_gauss.c`\n",
    "                    * detect point source in associated reconstructed image\n",
    "                    * under IFU/Snifs/pkg/focus/source/ \n",
    "                * `math_expr.y`\n",
    "                    * compute radius column from point source position\n",
    "                    * under IFU/IFU_gentools-6.4/pkg/math/source/ \n",
    "                * `sel_table.c`\n",
    "                    * select a region in the cube\n",
    "                    * under IFU/IFU_gentools-6.4/pkg/tools/source/\n",
    "                * `sum_aperture.c`\n",
    "                    * spatial (optimal) summation of datacube spectra\n",
    "                    * under IFU/IFU_gentools-6.4/pkg/tools/source/ \n",
    "                * `cp_nonstd_desc.c`\n",
    "                    * copy non standard descriptors between two files\n",
    "                    * under IFU/IFU_gentools-6.4/pkg/tools/source/ \n",
    "                * `wr_desc.c`\n",
    "                    * keywords copy from one channel to the other\n",
    "                    * under IFU/IFU_gentools-6.4/pkg/tools/source/ \n",
    "        * `fits_to_yaml.py`\n",
    "            * Purpose: Dump some of the fits HEADER key/value into the yaml post-processing log.\n",
    "            * Under SNFactory/Tasks/Processing/scripts/)            \n",
    "        * `quick_plot.py`\n",
    "            * Purpose: Quick spectrum plot.\n",
    "            * Under IFU/Snifs/pkg/pipeline/quick/\n",
    "* **Inputs**: Preprocessed files from `plan_file_quality`\n",
    "    * 4 / 207: Preprocessed arc frame with bias and dark\n",
    "    * 8 / 0: Preprocessed continuum frame\n",
    "    * 8 / 207: Preprocessed continuum frame with bias and dark\n",
    "    * 13 / 207: Preprocessed twilight frame with bias and dark\n",
    "    * 18 / 207: Preprocessed object frame with bias and dark\n",
    "    * 54 / 207: Preprocessed dome frame with bias and dark\n",
    "    * 61 / 0: Preprocessed SCALA frame\n",
    "    * 61 / 207: Preprocessed SCALA frame with bias and dark\n",
    "* **Outputs**: Fclass / XFclass : description\n",
    "    * 22 / 0: Reduced object cube [Euro3D]\n",
    "    * 38 / 0: Point-source spectrum - quick_extract\n",
    "    * 45 / 0: Reduced twilight cube [Euro3D]\n",
    "    * 58 / 0: Reduced dome cube [Euro3D]\n",
    "    * 65 / 0: Reduced SCALA cube [Euro3D]\n",
    "    * 300 / 1: Continuum cube table [Tiger]\n",
    "    * 322 / 0: fit_background bkg pixel table\n",
    "* **Comments**: \n",
    "    * Here are examples of [.sh](http://snf.in2p3.fr/file/31954968/content/), [.err](http://snf.in2p3.fr/file/32066757/content/) and [.out](http://snf.in2p3.fr/file/32066756/content/) files of a [typical PFS job](http://snf.in2p3.fr/job/245402/).\n",
    "* **CVS link**: [plan_cube_generation.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_cube_generation.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_extract_star`\n",
    "\n",
    "* **Purpose**: Point source object extraction and spectra production. Is also run on host-galaxy subtracted cubes after `plan_cubefit`.\n",
    "* **Who**:  Person in charge of the production\n",
    "* **When**: After `plan_cube_generation`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/PES\n",
    "* **What**:\n",
    "    * Get the cubes, eihter for a given night or a given target\n",
    "        * Fclass / XFclass = 22 / 000 (raw cubes) or 260 / 820 (host-subtracted cubes from `cubefit`)\n",
    "        * Selection on Status (2), quality values, and Run.Kind (exclude FinalRef)\n",
    "        * Local copy of the cubes if asked\n",
    "    * Loop on the cube list and run\n",
    "        * If raw cube (Fclass=22) and R channel, truncate it to [5100,9700] (`truncate_cube.c`), and convert it into the right format (Euro3D, `convert_file.c`)\n",
    "        * Run `extract_star.py` on the current cube, with different options depending on the input\n",
    "            * StdStar: sky polynomial background degree of 0;\n",
    "            * SNe: prior on Seeing, Sky polynomial background degree of 1;\n",
    "            * SNe after cubefit: prior on seeing and input parameters from cubefit (position), sky polynomial background degree of 0.\n",
    "        * If asked (option --residuals), and for StdStar or SNe after host-galaxy subtraction only, run\n",
    "            * `test` (Unix command) to test the existence of the extracted spectrum\n",
    "            * `subtract_psf.py` to extract the PSF from the original cube (residual cube)\n",
    "            * `extract_fixed_star.py` to extract and store the residual spectrum\n",
    "            * If we are working on a host-subtracted cube\n",
    "                * get the flux calibrated cube (the parent cube)\n",
    "                * run `subtract_psf` on this cube and store the result (cube containing the host only)\n",
    "* **On**: A set of nights or targets\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/plan_extract_star.pdf)]:\n",
    "    * Django + DB codes + SNfObj library\n",
    "    * libProcessing.proc_select_quality\n",
    "    * `extract_star.py` [[g](figures/extract_star.pdf)]\n",
    "        * Purpose: 3D PSF-based point-source extractor\n",
    "        * Under: SNFactory/Offline/pySNIFS/apps/extract_star/\n",
    "        * Dependencies:\n",
    "            * pySNIFS, pySNIFS_fit, libExtractStar (SNFactory/Offline/pySNIFS/lib/)\n",
    "            * ToolBox.Atmosphere, ToolBox.Arrays, ToolBox.Misc, ToolBox.MPL (SNFactory/Offline/ToolBox/)\n",
    "            * libRecord.Accountant (SNFactory/Tasks/Processing/scripts/)\n",
    "                * depends on the `bitarray` python [library](https://pypi.python.org/pypi/bitarray/)\n",
    "    * `subtract_psf.py` [[g](figures/subtract_psf.pdf)]\n",
    "        * Purpose: Generate (and subtract) extract_star PSF cube from extracted spectrum\n",
    "        * Under: SNFactory/Offline/pySNIFS/apps/extract_star/\n",
    "        * Dependencies:\n",
    "            * pySnurp.Spectrum\n",
    "                * Purpose: Simple module providing few FITS-based I/O classes and other facilities\n",
    "                * Under: IFU/Snifs/pkg/pipeline/tools/\n",
    "                * Dependencies:\n",
    "                    * ToolBox (SNFactory/Offline/ToolBox/)\n",
    "            * pySNIFS, libExtractStar\n",
    "    * `extract_fixed_star.py` [[g](figures/extract_fixed_star.pdf)]\n",
    "        * Purpose: 3D fixed PSF/aperture extractor\n",
    "        * Under: SNFactory/Offline/pySNIFS/apps/extract_star/\n",
    "        * Dependencies:\n",
    "            * pySnurp.Spectrum (see above)\n",
    "            * pySNIFS, libExtractStar\n",
    "    * `truncate_cube.c`:       \n",
    "        * Purpose: cube truncation\n",
    "        * Under IFU/Snifs/pkg/calib/source/\n",
    "    * `convert_file.c` \n",
    "        * Purpose: Interface to Convert File from different data format\n",
    "        * Under IFU/IFU_gentools-6.4/pkg/tools/source/\n",
    "* **Inputs**: Extracted cubes from `plan_cube_generation`\n",
    "    * 22 / 0: Reduced object cube [Euro3D]\n",
    "    * After cubefit\n",
    "        * 23 / 12: Telluric-corrected, flux-calibrated cube [3D]\n",
    "        * 260 / 820: Cubefit - flux-calibrated host-subtracted cube [fits]\n",
    "* **Outputs**: Extracted spectra\n",
    "    * 38 / 100: Point-source spectrum - extract_star\n",
    "    * After cubefit\n",
    "        * 270 / 800: Cubefit/ES-PSF Host/Point-source-subtracted cube\n",
    "        * 270 / 810: Cubefit/ES-PSF Point-source-subtracted cube, contains the host\n",
    "        * 666 / 800: Cubefit - Point-source spectrum - extract_star\n",
    "* **Comments**: \n",
    "    * Here are examples of [.sh](http://snf.in2p3.fr/file/32067936/content/), [.err](http://snf.in2p3.fr/file/32068130/content/) and [.out](http://snf.in2p3.fr/file/32068129/content/) files of a [typical PES job](http://snf.in2p3.fr/job/245636/).\n",
    "* **CVS link**: [plan_extract_star.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_extract_star.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_multi_standard`\n",
    "\n",
    "* **Purpose**: Generate mean nightly extinction and flux solution from all standard stars in a night, both under photometric (P) and non-photometric (nP) hypotheses. Create multi-standard pipeline 'sh' script.\n",
    "* **Who**:  Person in charge of the production\n",
    "* **When**: After `plan_extract_star`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/MoreFlux/PMS\n",
    "* **What**: \n",
    "    * For a given night (Run.Year and Run.Day), get the target (Standard stars) list and their spectra\n",
    "        * Point source extracted spectra from extract_star (38_100)\n",
    "        * Run.Kind and Target.Kind == 'StdStar'\n",
    "        * Specify the point-source extraction job, or get the spectra with Status=2\n",
    "        * Apply some quality cuts\n",
    "            * libProcessing.proc_select_quality\n",
    "            * Quality <= 2 (and > 0)\n",
    "            * Qzelog <= 2\n",
    "        * Require successful quick_extract and good centering\n",
    "            * libProcessing.proc_select_quick_extract\n",
    "            * Pose.TargetF > 100\n",
    "            * Pose.TargetX(Y) in [-4, 4]\n",
    "        * Require un-interrupted guided expos, and stable expos in P-conditions\n",
    "            * libProcessing.proc_select_guiding\n",
    "            * Uninterrupted guided long exposures\n",
    "                * Pose.EffTime > 60\n",
    "                * Exp.Guide == 1\n",
    "                    * 0 unguided , 1 guided (good) , 2 poor guiding , 3 bad guiding.\n",
    "                    * Come from the analysis of the \\_vid.fits file header.\n",
    "                * Exp.Interrupt <= 1\n",
    "                    * 0 means no info, 1 un-interrupted exposure, interrupted otherwise.\n",
    "                    * Come from the analysis of the \\_vid.fits file header.\n",
    "            * Stable, according to the guide star stability (for the photometric case only)\n",
    "                * Exp.GuideS > 0.8 (see Mantis \\#1409)\n",
    "                * Exp.GuideF > 0.01\n",
    "            * Allow unguided short exposures (add processes [assing the following cuts)\n",
    "                * Pose.EffTime < 2\n",
    "                * Exp.Guide == 0\n",
    "        * Exclude HZ43 (bad) and Hiltner600 (double)\n",
    "    * Copy locally all the selected files\n",
    "    * Process telluric correction computation using all the spectra (B & R)\n",
    "        * Input is the list of spectra\n",
    "        * Output is a single fits file, the telluric correction\n",
    "        * `comp_telluric.py -o TELL_CORR.fits -g png (some plots) ALL_THE_FITS`\n",
    "    * Correct all the spectra from telluric features \n",
    "        * Input is the previoulsy computed telluric correction (fits file) and a single spectrum\n",
    "        * Output is the telluric corrected spectrum (and some plots)\n",
    "        * `apply_telluric.py -g png -e TELL_CORR.fits -o KSPEC.fits SPEC.fits`\n",
    "    * Extinction estimation\n",
    "        * Input is the list of telluric corrected spectra plus a list of telluric bands to be discarded \n",
    "            * default is `-B $IFU_PATH/Snifs/user/data/telluric_lines.fits:'H2Os'`\n",
    "        * Output is a single extinction fits file \n",
    "            * e.g., extN_16_008.fits or extP_16_008.fits for non photo or photometric cases\n",
    "        * Tho cases\n",
    "            * Photometric (--strictlyPhotometric, no per-std correction)\n",
    "            * Non-photometric\n",
    "            * Always do both in case the photometricty definition (used in later plans) changes\n",
    "        * `comp_extinction.py -o extN_NIGHT.fits -g png -B LIST.fits Kspec_*.fits` (or extP)\n",
    "    * Process flux solution computation in both photometric and non-photometric cases\n",
    "        * Input is the extinction fits file computed previoulsy (P or N), a list of telluric bands to discard, and the list of spectra to work on\n",
    "        * Output is a single flux solution fits file (and some plots)\n",
    "            * fxSolN_16_008.fits or fxSolP_16_008.fits for non photo or photometric cases\n",
    "        * `comp_calibration.py -g png -e extN_NIGHT.fits -B LIST.fits -o fxSolN_NIGHT.fits Kspec_*.fits`\n",
    "* **On**: A set of nights.\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/plan_multi_standard.pdf)]:\n",
    "    * Django + DB codes + SNfObj library\n",
    "    * libProcessing\n",
    "        * proc_select_quality\n",
    "        * proc_select_quick_extract\n",
    "        * proc_select_guiding\n",
    "        * read_nightIDs\n",
    "    * comp_telluric.py [[g](figures/comp_telluric.pdf)]\n",
    "        * Purpose: Compute a telluric correction spectrum by adjusting a template of the telluric components (02 and H2O) on all (or part of the) standard stars of the night.\n",
    "        * Under: IFU/Snifs/pkg/pipeline/telluric/\n",
    "        * Dependencies:\n",
    "            * pySnurp (get_channel, RefTable)\n",
    "            * ToolBox (Optimizer and MPL)\n",
    "            * libExtinction (ustr, createTable, StdStar, Night, etc.)\n",
    "    * apply_telluric.py [[g](figures/apply_telluric.pdf)]\n",
    "        * Purpose: Correct the telluric features of a spectrum using a telluric correction table.\n",
    "        * Under: IFU/Snifs/pkg/pipeline/telluric/\n",
    "        * Dependencies:\n",
    "            * pySnurp (read_spectrum)\n",
    "            * libExtinction (read_telluric, tellBands, telluricCorrection)\n",
    "            * pySNIFS (SNIFS_cube)\n",
    "            * ToolBox (Optimizer and MPL) \n",
    "    * comp_extinction.py [[g](figures/comp_extinction.pdf)]\n",
    "        * Purpose: Compute a single data table from standard stars spectra of a night to provide extinction & flux calibration spectra as well as a grey extinction value per standard star.\n",
    "        * Under: IFU/Snifs/pkg/pipeline/telluric/\n",
    "        * Dependencies:\n",
    "            * libExtinction (tellBandsFromTemplate, tellBands, group_args, StdStar, Night, etc.)\n",
    "            * ToolBox (Statistics, ReST, Astro.Coords, SkyCalc, MPL)\n",
    "    * comp_calibration.py [[g](figures/comp_calibration.pdf)]\n",
    "        * Purpose: Compute multi-std flux solution spectrum for the night from extinction table created by comp_extinction.\n",
    "        * Under: IFU/Snifs/pkg/pipeline/telluric/\n",
    "        * Dependencies:\n",
    "            * pySnurp (RefTable, Spectrum)\n",
    "            * libExtinction (LOG10, group_args, StdStar)\n",
    "            * ToolBox (ReST, MPL)\n",
    "* **Inputs**: Exctracted spetra\n",
    "    * 38 / 100: Point-source spectrum - extract_star\n",
    "* **Outputs**:\n",
    "    * 620 / 600: Multi-std telluric correction\n",
    "    * 625 / 600: Multi-std extinction (photometric)\n",
    "    * 625 / 610: Multi-std extinction (non-photometric)\n",
    "    * 630 / 600: Multi-std flux solution (photometric)\n",
    "    * 630 / 610: Multi-std flux solution (non-photometric)\n",
    "* **Comments**: \n",
    "    * Here are examples of [.sh](http://snf.in2p3.fr/file/33420444/content/), [.err](http://snf.in2p3.fr/file/33465101/content/) and [.out](http://snf.in2p3.fr/file/33465100/content/) files of a [typical PMS job](http://snf.in2p3.fr/job/248807/).\n",
    "    * This plan solely relies on StdStar PES-generated spectra for a given night. In particular, it does not rely on MFRs.\n",
    "    * There is no need for more than one standard star before `comp_extinction`, where an error is issued for single-std night.\n",
    "* **CVS link**: [plan_multi_standard.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_multi_standard.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_photometric_ratios` - a !!\n",
    "\n",
    "* **Purpose**: Generate script to run SnfPhot pipeline (photometric ratios creation) for a given list of target.\n",
    "* **Who**:  Person in charge of the production\n",
    "* **When**: Before `plan_flux_solution`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/PPR\n",
    "* **What**: \n",
    "    * The plan will basically loop on a list of targets (after defining this list if needed), and run `SnfPhotPipeline.IO.plan_jobs` for each of them. Main inputs for this function is the name of a target, and the name of the job. The plan is usually run target by target.\n",
    "    * `SnfPhotPipeline.IO.plan_jobs` will\n",
    "        * get the list of file to work on (multi-filters) using SnfPhotPipeline.DB.get_mf\n",
    "            * Apply a selection\n",
    "                * `Fclass=18`, `XFclass=0`, `Status=2`\n",
    "                * `Channel=1` (P channel)\n",
    "                * `Pose_FK__Exp_FK__Filter='mf'` (multifilter)\n",
    "                * `Pose_FK__Exp_FK__Run_FK__Type='SPECTRED'` (spectred run)\n",
    "                * `Pose_FK__Quality=1 (Pose quality)\n",
    "                * `Pose_FK__EffTime__gt = 40` (exposure time bigger than 40s)\n",
    "                * `Pose_FK__Exp_FK__Interrupt__lte = 1` (not interrupted)\n",
    "                * `Pose_FK__Exp_FK__GuideX__gt = 2048` (guiding on 2nd CCD)\n",
    "                * `Pose_FK__Raster = 'Full'` (not a guiding vignet)\n",
    "                * `Pose_FK__Exp_FK__Quality__lte=Pose_FK__Quality__lte=Quality__lte=1`\n",
    "            * Order the processes y seeing\n",
    "            * Bad seeing, and exposures with moon at the end of the list\n",
    "            * Put the photometric night at the begining of the list (keep the same order as above)\n",
    "            * Prevent rotated and/or noisy images as reference (don't really cut them, just place them last)\n",
    "            * Return the list of processes\n",
    "            * The first one of the list (photometric night, best seeing, no moon, no rotation) will be the one selected as a reference exposure while computing the MFRs.\n",
    "        * when the list of file is defined, the `SnfPhot` script, which actually is the main pipeline script for the SNf photometric channel, will be loaded, parsed, and modified to include this new list of files and the input options. We will now detail the content of this script, which generally looks like [that](http://snf.in2p3.fr/file/32072406/content/).\n",
    "        * \n",
    "            \n",
    "* **On**: A list of targets (or a list of nights in the incremental mode)\n",
    "* **DB**: YES.\n",
    "* **Dependencies**: \n",
    "    * Django + DB codes + SNfObj library\n",
    "    * SnfPhotPipeline\n",
    "        * Purpose: SnfPhotPipeline python package - python scripts for SNfactory's Photometric channel pipeline\n",
    "        * Under: SNFactory/Tasks/Photometry/scripts/\n",
    "        * Dependencies\n",
    "            * DB (needed for the incremental mode only)\n",
    "            * IO.plan_jobs\n",
    "                * Returns SnfPhot lines that plan_photometric_ratios will use for the job creation\n",
    "                * This function is actually the one creating the python script that we run on the worker\n",
    "            * `SnfPhot.py`\n",
    "                * Purpose: SNf photometric channel main pipeline script\n",
    "                * Under: SNFactory/Tasks/Photometry/scripts/\n",
    "                * Dependencies\n",
    "                    * Django + DB codes + SNfObj library\n",
    "                    * SnfPhotPipeline (DB, DbImage, IO, LC, Plot, Poloka, Prod)\n",
    "* **Inputs**:\n",
    "    * 18 / 0: Preprocessed object frame\n",
    "* **Outputs**:\n",
    "    * 111 / 0: Image without background\n",
    "    * 112 / 0: Image without background\n",
    "    * 113 / 0: Image without background\n",
    "    * 114 / 0: Image without background\n",
    "    * 115 / 0: Image without background\n",
    "    * 121 / 0: Image without background\n",
    "    * 122 / 0: Image without background\n",
    "    * 123 / 0: Image without background\n",
    "    * 124 / 0: Image without background\n",
    "    * 125 / 0: Image without background\n",
    "    * 131 / 0: Raw MFR file\n",
    "    * 131 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "    * 132 / 0: Raw MFR file\n",
    "    * 132 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "    * 133 / 0: Raw MFR file\n",
    "    * 133 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "    * 134 / 0: Raw MFR file\n",
    "    * 134 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "    * 135 / 0: Raw MFR file\n",
    "    * 135 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "    * 151 / 0: Image without background\n",
    "    * 152 / 0: Image without background\n",
    "    * 153 / 0: Image without background\n",
    "    * 154 / 0: Image without background\n",
    "    * 155 / 0: Image without background\n",
    "    * 191 / 0: Photometric reference stacks F1\n",
    "    * 192 / 0: Photometric reference stacks F2\n",
    "    * 193 / 0: Photometric reference stacks F3\n",
    "    * 194 / 0: Photometric reference stacks F4\n",
    "    * 195 / 0: Photometric reference stacks F5\n",
    "* **Comments**: \n",
    "    * This plan is mainly based on the SnfPhotPipeline library, of which a documentation can be found at the following [link](http://snovae.in2p3.fr/pereira/SnfPhotPipeline/). However, this documentation is not up to date. As an example, Tools.solve_mfr_matrix is missing (last update Junuary 2011).\n",
    "    * Rui's thesis is available online [here](https://tel.archives-ouvertes.fr/tel-00372504v2/document) (see also [here](https://tel.archives-ouvertes.fr/tel-00372504)). Documentation on MFR are available on the following links: [1](figures/MFRcalibration.pdf), [2](figures/photospec.pdf).\n",
    "    * We deliberately ignored the incremental mode of this plan, which, for an existing target with existing MFRs, will compute the MFRs of new exposures of the same target using the previously selected reference exposure of the given target.\n",
    "    * Here are examples of [.py](http://snf.in2p3.fr/file/32072406/content/), [.err](http://snf.in2p3.fr/file/32190279/content/) and [.out](http://snf.in2p3.fr/file/32190276/content/) files of a [typical PMS job](http://snf.in2p3.fr/job/246257/).\n",
    "* **CVS link**: [plan_photometric_ratios.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_photometric_ratios.py)\n",
    "\n",
    "### `plan_photometric_ratios` - b !!\n",
    "\n",
    "* **Purpose**: Compute the MFR scale factor\n",
    "* **Who**:  Person in charge of the production\n",
    "* **When**: Between `tabPhotometricity` `plan_flux_solution`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/PPR/SF\n",
    "* **What**:\n",
    "    * In its \"scale_factor\" mode, PPR is actually writing a very simple python script containing the following piece of code\n",
    "            \n",
    "            \"\"\"\n",
    "            SOME HEADER USED BY THE WORKER\n",
    "            \n",
    "            from SnfPhotPipeline import Tools\n",
    "            mfr = {}\n",
    "            tgt = []\n",
    "            kwargs = %s\n",
    "            Tools.solve_mfr_matrix(mfr, tgt, output='%s', verbose=True, status=%d, **kwargs)\n",
    "            Tools.solve_mfr_matrix(mfr, tgt, output='%s', verbose=True, method='aperture', status=%d, **kwargs)\n",
    "            \n",
    "            SOME FOOTER FOR DB REGISTRATION\n",
    "            \"\"\"\n",
    "            (kwargs, output, opts.status, output, opts.status)\n",
    "            \n",
    "    * `kwargs` includes \n",
    "        * the name of the jobs that computed the MFRs (e.g., SNF-0203-NEWMFR),\n",
    "        * a code version (e.g., 203),\n",
    "        * a list of target to exclude,\n",
    "        * the minimal number of night that a target have been oberved to get including in the list of target for which a scale factor will be computed (currently 2).\n",
    "    * `output` is a file name in which the scale factor will be saved, or 'DB' if the results have to be saved in the DB instead.\n",
    "    * `status` if the Process.Status value, used to selected the \"good\" processes in the DB.\n",
    "    * The main work is then done by SnfPhotPipeline.Tools.solve_mfr_matrix\n",
    "    \n",
    "* **On**: All targets with MFRs\n",
    "* **DB**: YES.\n",
    "* **Dependencies**: \n",
    "    * SnfPhotPipeline.Tools.solve_mfr_matrix\n",
    "        * Purpose: Solve the MFR matrix for the scale factors needed to make night-to-night MFR comparable\n",
    "        * Under: SNFactory/Tasks/Photometry/scripts/SnfPhotPipeline/Tools.py\n",
    "* **Inputs**:\n",
    "    * 132 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "    * 133 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "    * 134 / 100: DB-stored airmass corrected MFR (mean extinction)\n",
    "* **Outputs**:\n",
    "    * 136 / 0: Kernel MFR scale factor\n",
    "    * 136 / 1: Aperture MFR scale factor\n",
    "* **Comments**:\n",
    "    * Here are examples of the [.py](http://snf.in2p3.fr/file/34286686/content/) file of a [typical PMS job](http://snf.in2p3.fr/job/256447/). There is no .err or .out files associated to these jobs.\n",
    "* **CVS link**: [plan_photometric_ratios.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_photometric_ratios.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tabPhotometricity` \n",
    "\n",
    "* **Purpose**:  Create (or complete) the photometricity table and DB using various available information sources (CFHT SkyProbe, multi-filter ratios, multi-standard grey extinction and guide star flux stability).\n",
    "* **Who**: Person in charge of the production\n",
    "* **When**: After `plan_multi_standard`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/MoreFlux/\n",
    "* **What**:\n",
    "    * Uses different sources to estimate the photometricity quality of a night. A night will finaly be fully photometric or non-photometric, according to this analysis. See the comments section for information about the photometricity override.\n",
    "    * There are many ways (options) to get the list of night to work on\n",
    "        * year: a full year will be processed.\n",
    "        * day: process a specific day (in a year).\n",
    "        * yrange: process an year range.\n",
    "        * from_day: data taking day to start the Photometricity analysis.\n",
    "        * to_day: data taking day to stop the Photometricity analysis.\n",
    "        * nights_list: text file including list of night names (Only one year at a time!).\n",
    "        * guess: try to guess the days range (in the current year) to increment.\n",
    "    * For a given list of year/day, it will loop over the nights with observations and work at the night level, running the following analyses.\n",
    "    * It will by default make png plots and store them in a specific directory (see the comments section), along with a table containing the main results of the analysis.\n",
    "    * If no data are found for a given night, it will pass. otherwise, it will performe the analysis.\n",
    "    * **libPhotometricity.ObsInfo**\n",
    "        * Retrieve various informations of a given night related to the photometricity study.\n",
    "        * Reject nights without useful data. Define all keywords needed for the analysis.\n",
    "        * Initialisation of the night object, used later with other inputs to define the photometricity of the night.\n",
    "        * It will get info on\n",
    "            * Observation time\n",
    "            * Position in the sky\n",
    "            * Event type\n",
    "            * Guide star info (guided or not)\n",
    "            * LunSky info\n",
    "    * **libPhotometricity.SkyProbe**\n",
    "        * Retrieve CFHT SkyProbe informations. Plot atmopsheric transmission and fill accordingly the photometricity table.\n",
    "        * SnfParseLog.FnameLog(year, day, 'skyprobe') is used to get the SkyProbe log of the night\n",
    "        * One example of such a file \n",
    "        \n",
    "                /sps/snovae//SRBregister/log/15/089/2015-03-30.SkyProbe.dat\n",
    "                \n",
    "                START_TIME           FILTER     ZP_OBS   ZP_ERR  RA          DEC    C_AIRMASS SKY     NSTAR \n",
    "                 2015/03/30,05:07:56  SKYPROBE.V -0.0200  0.0240  36.795804  60.229836  1.766  32767.0 27    \n",
    "                 2015/03/30,05:08:55  SKYPROBE.V -0.0060  0.0170  36.796628  60.228498  1.772  32767.0 27    \n",
    "                 2015/03/30,05:09:56  SKYPROBE.V -0.0120  0.0180  36.829928  60.222406  1.778  32767.0 46    \n",
    "                 2015/03/30,05:10:55  SKYPROBE.V 0.0050   0.0160  36.808895  60.228263  1.784  32767.0 49    \n",
    "                 2015/03/30,05:11:57  SKYPROBE.V -0.1010  0.0150  36.787120  60.231689  1.788  32767.0 62    \n",
    "                 2015/03/30,05:12:56  SKYPROBE.V -0.0380  0.0110  36.799421  60.226760  1.794  32767.0 78    \n",
    "                 2015/03/30,05:13:56  SKYPROBE.V -0.0150  0.0110  36.789167  60.229701  1.800  32767.0 98    \n",
    "                 2015/03/30,05:14:56  SKYPROBE.V -0.0750  0.0100  36.795052  60.226718  1.806  32767.0 12\n",
    " \n",
    "        * Which is then parsed, cleaned (pathologic points, changed pointing, sky level, before/after 12d twiligh), and analysed\n",
    "        * The third/fouth columns are the extinction and extinction error. The transmission is comptued from their values, and its stability (mean, std, nmad) over the night is estimated for\n",
    "            * all (cleaned) SkyProbe data\n",
    "            * all SkyProbe data between first and last SNIFS exposure\n",
    "            * all SkyProbe data taken during SNIFS exposures\n",
    "        * These numbers will then be used later to estimate the photometricity of the night\n",
    "    * **libPhotometricity.MultiStandard**\n",
    "        * Multi-standard (long exposures) transmission stability.\n",
    "        * Retrieve multi-standard extinction 'fits' files (Fclass=625, XFclass=610)\n",
    "        * Select long exposures standard stars only.\n",
    "        * Compute statistics of the multi-standard transmission stability over the night (mean, rms, nmad)      \n",
    "    * **libPhotometricity.MultiFilter** (ratios)\n",
    "        * Multi-filter ratios transmission stability.\n",
    "        * Retrieve rescaled multi-filter ratios from DB (kernel values)\n",
    "        * Comput ethe mean, rms and nmad of the MFR distribution over the night\n",
    "    * **libPhotometricity.Telescope**\n",
    "        * Retrieve telescope and SNIFS informations using SnfParseLog.SnfVar, e.g.,\n",
    "        \n",
    "                 /sps/snovae/SRBregister/log/15/089/log_var.txt\n",
    "                 \n",
    "                 #UTC_time  Altitude  Azimuthal_Angle ERR_1  ERR_2  UT_1   UT_2   UT_3   INCANDESCENTS_L FLOURESCENTS_L  Temperature     Humidity\n",
    "                 1427673600.11   90.0    180.0   0.0     167.5   23      59      09              []      []      3.5     2.1\n",
    "                 1427673605.35   90.0    180.0   0.0     167.5   23      59      15              []      []      3.5     2.1\n",
    "                 1427673610.56   90.0    180.0   0.0     167.5   23      59      20              []      []      3.5     2.1\n",
    "                 1427673615.11   90.0    180.0   0.0     167.5   23      59      24              []      []      3.5     2.1\n",
    "        \n",
    "        * Extract SNIFS humidity and temperature as well as telescope informations from this file (airmass and dome errors)\n",
    "        * These will be used later\n",
    "    * **libPhotometricity.GuideStar**\n",
    "        * Retrieve guide star data files. Clean the data points, and compute some stability statistics.\n",
    "        * Get statistics informations on guide star stability for each guided event and for the whole nigh and assess the quality of the guiding\n",
    "        * DB search  of the GS  file(s) for the full  night or for a specific pose (AuxFile of raw data, Fclass=17, XFclass=2)\n",
    "        * An example of such a file\n",
    "        \n",
    "                /sps/snovae/SRBregister/hawaii/15/089/15_089_055_002_17_P_gs.txt\n",
    "                \n",
    "                 ntelave = 5, gstime = 2.000, telgain = 0.800\n",
    "                 pixscale = 0.137, patop = 179.700, eccwn = 0\n",
    "                 psfalgo = 1, telfixorigin = 1, nsid_guiding = 0, nsid_ra_rate = 0.0000, nsid_dec_rate = 0.0000, nsid_x_rate = 0.0000, nsid_y_rate = 0.0000\n",
    "                  Iter   Time     UT     Exp   ReadLate UnixLate    x CCD-3 y      S/N     Flux  FWHM    x--pred--y      gsx     gsy    gszx    gszy    gdzp_x  gdzp_y\n",
    "                    0   2.4050 11:19:09 2.0000   0.4050   0.0000  3111.0  1020.0    84.2   10283 10.50     0.0     0.0  3112.8  1019.4  3112.8  1019.4     0.0     0.0\n",
    "                    1   4.8177 11:19:11 2.0006   0.4122   0.0006  3111.4  1020.9    79.3   10712  8.69  3111.0  1020.0  3112.8  1019.4  3112.8  1019.4  3112.8  1019.4\n",
    "                    2   7.2305 11:19:14 2.0004   0.4123   0.0004  3111.6  1021.6    74.5   10800 10.21  3111.4  1020.9  3112.8  1019.4  3112.8  1019.4  3112.8  1019.4\n",
    "                    3   9.6438 11:19:16 2.0005   0.4129   0.0005  3111.0  1021.6    62.6   10596 10.02  3111.6  1021.6  3111.4  1022.0  3112.8  1019.4  3112.8  1019.4\n",
    "                    4  12.0572 11:19:18 2.0005   0.4129   0.0005  3110.7  1021.4    95.1   11291  8.79  3111.0  1021.6  3111.3  1021.6  3112.8  1019.4  3112.8  1019.4\n",
    "                    5  14.4704 11:19:21 2.0004   0.4128   0.0004  3111.4  1021.6   147.8    9953  7.99  3110.7  1021.4  3111.0  1021.0  3112.8  1019.4  3112.8  1019.4\n",
    "                \n",
    "    * **libPhotometricity.Weather**\n",
    "        * Retrieve weather informations from UH and CFHT meteo stations.\n",
    "        * Actually not used to estimate the photometricity of the night (only for plotting purposes)\n",
    "    * Combine all the statistics previoulsy estimated to get the photometricty of the night (see slides).\n",
    "* **On**: A set of night\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/tabPhotometricity.pdf)]:\n",
    "    * Django + DB codes + SNfObj library\n",
    "    * ToolBox.MPL\n",
    "    * libPhotometricity.py\n",
    "        * Purpose: Library for photometricity analyses.\n",
    "        * Under: SNFactory/Tasks/Calibration/Reftables/Photometricity/Tools/\n",
    "* **Inputs**: \n",
    "    * Nothing from the SNf DB, according to the DB parenting, but...\n",
    "    * Several log files, as well as info from the DB (Run.???, gs.txt files)\n",
    "* **Outputs**: \n",
    "    * 995  000  SkyProbe photometricity data\n",
    "* **Comments**:\n",
    "    * An analysis made by C. Buton in 2011is availalbe [here](http://snovae.in2p3.fr/snprod/PhotoNight/Analysis/statistics.pdf). It also includes the criteria used to define the photometricity of a night for the different sources and for the overall analysis.\n",
    "    * The standard way to use this script is in the append (to the already existing photometricity table) and guess (what was the last day processed) modes\n",
    "    \n",
    "            tabPhotometricity --append --guess --DB --versionDB 203\n",
    "            \n",
    "    * All the outplots and tables used to define the photometricity of a night are stored at CC-IN2P3 under\n",
    "        * `/afs/in2p3.fr/group/snovae/snprod/PhotoNight`\n",
    "    * These outputs are accessible on your web browser at\n",
    "        * http://snovae.in2p3.fr/snprod/PhotoNight/ for all year\n",
    "        * http://snovae.in2p3.fr/snprod/PhotoNight/2015 for a specific year\n",
    "    * Photometricity override\n",
    "        * The Fclass/XFclass of the photometricity override process is\n",
    "            * 995  042  Photometricity overrides\n",
    "        * An override can be added as followed\n",
    "\n",
    "                from SnfPhotometricity import add_override \n",
    "                add_override(year, day, photometric, comment, who=None, version=None)\n",
    "                \n",
    "        * `year` and `day` are integers\n",
    "        * `photometric` is a boolean (True of False)\n",
    "        * `comment` is a string, a comment added by the user\n",
    "        * `who` is a string, the user name\n",
    "        * `version` is the DB version, i.e., 203.\n",
    "            \n",
    "* **CVS link**: [tabPhotometricity.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Calibration/Reftables/Photometricity/Tools/tabPhotometricity.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_flux_solution`\n",
    "\n",
    "* **Purpose**: Generate the flux solution and atmospheric extinction for each exposure of a night, both under P and nP hypotheses.\n",
    "* **Who**:  Person in charge of the production\n",
    "* **When**: After `plan_multi_standar` + `plan_photometric_ratios` + `tabPhotometricity`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/MoreFlux/PFS\n",
    "* **What**:\n",
    "    * Only runs on nights containing SPECTRED StdStar\n",
    "    * First get the targets and the associated files for the considered night\n",
    "        * Only get StdStar and SuperNova selected with their Run.Kind\n",
    "        * Need a PPR job name to get the right MFRs\n",
    "        * Queries used to get the StdStars\n",
    "            * From the last official PES production (Fclass=38, XFclass=100)\n",
    "            * Same quality cuts as in plan_multi_standard\n",
    "            * For the selected processes, get a list of info used later\n",
    "                * Target names\n",
    "                * Specrum path\n",
    "                * Spetrum name,\n",
    "                * Exposure open time\n",
    "                * Has MFR (boolean). We want the rescaled (kernel) MFR, using the input PPR job name\n",
    "                * Obs. Id\n",
    "                * Year, Day\n",
    "                * Object kind\n",
    "                * Is phot (boolean)\n",
    "                * Plus a array of 4 empty values to be filled later with MFR info\n",
    "        * Queries to get the SuperNova\n",
    "            * Target.Kind must be 'SuperNova' or 'Candidate'\n",
    "            * Same quality cuts as above\n",
    "            * Same parameter selectino as for the standard stars\n",
    "        * When the list of target is ready, get the MFR values of these targets\n",
    "            * This uses SnfPhotPipeline.IO.get_photometricratios, which returns one big dictionnary including all targets, and all of their spectra. We are looking for airmass corrected, rescaled MFR here.\n",
    "            * This dictionnary will also be saved in a file (MFRfile below), which will be used as input for the two scripts used later, `adjust_extinction` and `rescale_flux`).\n",
    "            * Check the MFR availability of the selected processes. If a given spectrum does not have an MFR, skip it.\n",
    "            * If we have an MFR, complete the targets dictionnary with MFR info (the last 4 empty slots)\n",
    "                * preprocessed P-channel frame ID used for MFRs.\n",
    "                * the MFRs process IDs\n",
    "        * When the targets/processes selection is done, return the main dictionnary filled with all the need info\n",
    "    * Then get the multi-std analysis results using the PMS job name given by the user, needed for flux calibration of targets.\n",
    "        * Get non-P flux solutions for nights during which targets where observed (B + R)\n",
    "            * Fclass = 630\n",
    "            * XFclass = 610\n",
    "            * PMS job name\n",
    "        * Get non-P extinctions for nights during which targets where observed\n",
    "            * Fclass = 625\n",
    "            * XFclass = 610\n",
    "            * PMS job name\n",
    "        * Get reference extinctions for targets\n",
    "            * Fclass = 625\n",
    "            * XFclass = 600\n",
    "            * PMS job name\n",
    "        * Return these three processes for the given night\n",
    "    * Copy the different inputs from the DB to the worker\n",
    "        * the multi-standard analysis results\n",
    "            * 625_600 (extP_YY_DDD.fits)\n",
    "            * 625_610 (extN_YY_DDD.fits)\n",
    "            * 630_610 (fxSolN_YY_DDD_R.fits)\n",
    "            * 630_610 (fxSolN_YY_DDD_B.fits)\n",
    "        * the (rescaled) MFR file ('kernel' method) using the given MFR job\n",
    "            * SNF-0203-NEWYORKx-YY_DDD.mfr (the MFRfile)\n",
    "    * And finaly, generate the command lines to do flux calibration from multi-stantard stars.\n",
    "        * If there is a multi-standard extinction, take it and go on, there is nothing left we can do for this night otherwise\n",
    "        * Get the multi-standard flux solution, which will be the input of `rescale_flux.py`\n",
    "        * Loop over the targets of the night and run `adjust_extinction` for all of them\n",
    "            * If no MFR, skip it (rescaled MFR are now always used, so there is no need to use the reference MFR)\n",
    "            * if everything is there, run\n",
    "            \n",
    "                    adjust_extinction --ratios MFRfile --nExt extN_YY_DDD.fits \n",
    "                                      --nRun YY_DDD_RRR_EEE --output ext_YY_DDD_RRR_EEE.fits\n",
    "                    \n",
    "        * And at the end, run rescale_flux on both channels\n",
    "        \n",
    "                    rescale_flux --graph rescale_flux_12_190.png --ext extN_12_190.fits \n",
    "                                 --ratios MFRfile fxSolN_12_190_R.fits fxSolN_12_190_B.fits \n",
    "                                 \n",
    "* **On**: A set of nights.\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/plan_flux_solution.pdf)]:\n",
    "    * SnfPhotPipeline.IO\n",
    "    * Django + DB codes + SNfObj library\n",
    "    * libProcessing\n",
    "    * adjust_extinction.py \n",
    "        * Purpose: Derive atmospheric extinction in non-phot. conditions\n",
    "        * Under: IFU/Snifs/pkg/pipeline/flux/\n",
    "        * Dependencies [[g](figures/adjust_extinction.pdf)]:\n",
    "            * pyRatios (to read MFr files and get the MFR values)\n",
    "            * pySnurp (RefTable)\n",
    "            * libExtinction (createTable to create a FITS-table from a set of array)\n",
    "    * rescale_flux.py \n",
    "        * Purpose: Renormalize flux solution from MF-ratios\n",
    "        * Under: IFU/Snifs/pkg/pipeline/flux/\n",
    "        * Dependencies [[g](figures/rescale_flux.pdf)]:\n",
    "            * pyRatios (to read MFr files and get the MFR values)\n",
    "            * pySnurp (RefTable)\n",
    "            * ToolBox.Statistics (median, weighted mean, pearson correlation)\n",
    "* **Inputs**:\n",
    "    * 18 / 0: Preprocessed object frame\n",
    "    * 132 / 0: Raw MFR file\n",
    "    * 133 / 0: Raw MFR file\n",
    "    * 134 / 0: Raw MFR file\n",
    "    * 625 / 610: Multi-std extinction (non-photometric)\n",
    "    * 630 / 610: Multi-std flux solution (non-photometric)\n",
    "* **Outputs**:\n",
    "    * 625 / 700: MFR adjusted multi-std extinction\n",
    "    * 630 / 700: MFR adjusted multi-std flux solution\n",
    "* **Comments**:\n",
    "    * For a given night, this plan uses mean extinction and flux solution (generated by PMS) and MFRs (from PPR) to derive exposure-specific extinction and MFR-rescaled nightly flux solution. If MFRs were to be modified (e.g. change of scaling factor), PFS would need to be run again on all exposures. \n",
    "    * Two flux solutions will be computed, one assuming photometric conditions and the other non-photometric conditions. The choice of the right solution to be picked up will be done at a latter stage (`plan_flux_calibration`).\n",
    "    * Here are examples of [.sh](http://snf.in2p3.fr/file/33697909/content/), [.err](http://snf.in2p3.fr/file/33701982/content/) and [.out](http://snf.in2p3.fr/file/33701981/content/) files of a [typical PFS job](http://snf.in2p3.fr/job/250940/).\n",
    "    * Usually used as followed (VERISON=203 and NAME=NEWYORK in the current production):\n",
    "        \n",
    "            plan_flux_solution -p SNF-0VERSION-NAME --multistd -m SNF-0VERSION-NAME \n",
    "                               --rescaledMFR -r SNF-0VERSION --outver VERSION YY_DDD\n",
    "        \n",
    "* **CVS link**: [plan_flux_solution.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_flux_solution.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_flux_calibration`\n",
    "\n",
    "* **Purpose**: Use exposure-specific flux solutions and extinctions to calibrate exposures (cubes or spectra), assuming a given photometricity (P|nP).\n",
    "* **Who**:  Person in charge of the production\n",
    "* **When**: After `plan_flux_solution`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/MoreFlux/PFC\n",
    "* **What**:\n",
    "    * First, get the target name, the PFS job name, and the PMS job name\n",
    "    * Get the corresponding exposures\n",
    "        * For the studied target \n",
    "        * For the B or R channel\n",
    "        * Fclass = 17\n",
    "        * 'SPECTRED' runs\n",
    "    * Get the photometricity info for all nights\n",
    "    * Loop over the exposures\n",
    "        * Get the corresponding night\n",
    "        * Get its photometricity\n",
    "        * Atmospheric extinction (channel independant)\n",
    "            * If the night is photometric, get the photometric nightly multi-std extinction from PMS (Fclass=625, XFclass=600)\n",
    "            * If it is non-photometric, get the non-photo multi-stdandard MFR-adjusted extinction from PFS (Fclass=625, XFclass=700)\n",
    "            * If non-photo and no extinction found, allow for relative flux calibration and get the non MFR-adjusted extinction from PMS (Fclass=625, XFclass=610). The relatively frlux-calibrated spectra/cubes will thus be flagged in the DB as PFC_RELFLX.\n",
    "            * If no extinction found, skip the current exposure. It will not be flux calibrated.\n",
    "            * Copy the extinction file locally if one is found\n",
    "        * Telluric correction (channel independant)\n",
    "            * Get the nightly multi-std telluric correction from PMS (Fclass=620, XFclass=600)\n",
    "            * If no telluric correction found, skip the current exposure. It will not be flux calibrated.\n",
    "            * Copy the telluric correction locally if one is found.\n",
    "        * Flux solution (per channel)\n",
    "            * Loop over the B and R channels\n",
    "            * Look for the flux solution of the current night (Channel=2|4 (R or B), Fclass=630, XFclass=600|700 (P or NP))\n",
    "            * If no flux solution found, look for a flux solution from a neighboring night with both channels (+- 20 days around the studied night)\n",
    "            * Copy the selected flux solution\n",
    "        * Get the cubes corresponding to the current exposure          \n",
    "            * Fclass=22, XFclass=0, Status=2, Channel=2|4\n",
    "            * Apply generic quality cuts on input files\n",
    "            * DO NOT require successful quick_extract and good centering: we want to flux calibrate everything\n",
    "            * Select unguided short exposures or uninterrupted guided long exposures, with further request of stable GS in photometric conditions, to discard e.g. dome vignetted exposures\n",
    "            * Flux calibrate it, including truncation and flat-fielding if needed\n",
    "                * Get the .tig format cube and associated .fits file\n",
    "                * Define the name of the output files\n",
    "                * Copy all the input files from the DB\n",
    "                * Check if we are in the inter-night flux calibration case. If so, we will need to run some preprocessing commands on the cube before running the generic flux calibration commands\n",
    "                    * Get the .tig format cube\n",
    "                    * Look for current night flat-field cube\n",
    "                    * Un-flat-field the current cube\n",
    "                        * apply_lfff -reverse \n",
    "                    * Look for flat-field cube from flux calibration night\n",
    "                    * Flat-field the cube\n",
    "                        * apply_lfff\n",
    "                    * Output tigname is 'F'+tigname at the end of this process\n",
    "                * Truncate the cube for the R channel\n",
    "                    * `truncate_cube -in TIGNAME -out TTIGNAME -wave 5100,9700`\n",
    "                    * wavelength limits are given as inputs to the plans\n",
    "                * Apply the flux calibration\n",
    "                    * `apply_flux -flux FLUXSOLUTION -extinct EXTINCTION -in TIGNAME -out XTIGNAME\n",
    "                    * the input is the TIGNAME cube (or the truncated one for the R channel)\n",
    "                * Convert flux calibrated cubes from tiger to euro3d format\n",
    "                    * `convert_file -inputformat \"tiger+fits\" -outputformat \"euro3d\" -in XTIGNAME -out XE3DNAME`\n",
    "                * If this an inter-night flux-calibration, or a relative flux calibration, flag the output processes using the corresponding flag ('PFC_XNIGHT' or 'PFC_RELFLX').\n",
    "                * Apply the telluric correction\n",
    "                    * `apply_telluric --extinction TELLNAME --output KE3DNAME XE3DNAME`\n",
    "                * Convert from E3D to plain 3D format for later convenience\n",
    "                    * + `e3dto3d.py -o K3DNAME KE3DNAME`\n",
    "        * Get the spectra corresponding to the current exposure \n",
    "            * Fclass=38, XFclass=100, Status=2, Channel=2|4\n",
    "            * Apply the same quality cuts as above\n",
    "            * Flux calibrated it, as well as the corresponding sky spectrum (XFclass=110)                \n",
    "                * run `apply_flux`\n",
    "                    * `apply_flux -spec -flux FLUXSOLUTION -extinct EXTINCTION -in THESPEC -out XTHESPEC\n",
    "                    * `-spec` mode to run on a spectrum\n",
    "                    * 'XTHESPEC' is the name of the output frlux calibrated spectrum, used as input in `apply_telluric`.\n",
    "                * run `apply_telluric`\n",
    "                    * `apply_telluric --extinction TELLNAME --output KTHESPEC XTHESPEC`\n",
    "                    * 'KTHESPEC' is the flux-calibtrated and telluric-corrected spectrum (coulb be relative flux calibration)\n",
    "            * Inter-night flux calibration does not apply to spectra: check that input specproc and fluxproc are coming from the same night)\n",
    "        * If the target is a standard star, run `check_flux`\n",
    "* **On**: A set of targets.\n",
    "* **DB**: YES.\n",
    "* **Dependencies** [[g](figures/plan_flux_calibration.pdf)]:\n",
    "    * `SnfPhotometricity.NightPhotometricities` (to get photometricity info on a night)\n",
    "    * `SnfUtil.expand` (Expand a run, exposure, pose, or process ID to include underscores)\n",
    "    * `SnfQuery` (for easier DB queries)\n",
    "    * and other `SnfObj` scripts (`SnfMonitor`, `SnfTask.Task`, `SnfCommand.GenericCommand`, `SnfWrite.ProcessVersion`)\n",
    "    * `libProcessing`\n",
    "    * `truncate_cube.c`\n",
    "        * Purpose: cube truncation\n",
    "        * Under: IFU/Snifs/pkg/calib/source/\n",
    "    * `apply_flux.c`\n",
    "        * Purpose: apply flux calibration\n",
    "        * Under: IFU/Snifs/pkg/calib/source/\n",
    "    * `sel_table.c`\n",
    "        * Puprose: select a region in the cube\n",
    "        * Under IFU/IFU_gentools-6.4/pkg/tools/source/\n",
    "    * `convert_file.c`\n",
    "        * Purpose: Interface to Convert File from different data format\n",
    "        * Under: IFU/IFU_gentools-6.4/pkg/tools/source/\n",
    "    * `apply_telluric.py`\n",
    "        * Purpose: Correct the telluric features of a spectrum using a telluric correction table.\n",
    "        * Under: IFU/Snifs/pkg/pipeline/telluric/\n",
    "        * Dependencies [[g](figures/apply_telluric.pdf)]:\n",
    "            * `pySnurp` (read_spectrum)\n",
    "            * `libExtinction` (read_telluric, tellBands, telluricCorrection)\n",
    "            * `pySNIFS` (SNIFS_cube)\n",
    "            * `ToolBox` (Optimizer and MPL)\n",
    "    * `e3dto3d.py`\n",
    "        * Purpose: Convert cube from euro3D to 3D\n",
    "        * Under: SNFactory/Offline/pySNIFS/apps/extract_star/\n",
    "        * Dependencies:\n",
    "            * `pySNIFS.SNIFS_cube`\n",
    "    * `check_flux.py`\n",
    "        * Purpose: Compare flux-calibrated std star spectrum to reference table (makes plots)\n",
    "        * Under: IFU/Snifs/pkg/pipeline/tools/\n",
    "        * Dependencies:\n",
    "            * `pySnurp`\n",
    "            * `libExtinction`\n",
    "            * `ToolBox.MPL`\n",
    "* **Inputs**:\n",
    "    * 22 / 0: Reduced object cube [Euro3D]\n",
    "    * 38 / 100: Point-source spectrum - extract_star\n",
    "    * 620 / 600: Multi-std telluric correction\n",
    "    * 625 / 600: Multi-std extinction (photometric)\n",
    "    * 625 / 610: Multi-std extinction (non-photometric)\n",
    "    * 625 / 700: MFR adjusted multi-std extinction\n",
    "    * 630 / 600: Multi-std flux solution (photometric)\n",
    "    * 630 / 700: MFR adjusted multi-std flux solution\n",
    "    * 995 / 0: SkyProbe photometricity data\n",
    "    * 995 / 42: Photometricity overrides\n",
    "* **Outputs**:\n",
    "    * 23 / 0: Flux-calibrated cube [Euro3D]\n",
    "    * 23 / 10: Telluric-corrected, flux-calibrated cube [Euro3D]\n",
    "    * 23 / 12: Telluric-corrected, flux-calibrated cube [3D]\n",
    "    * 640 / 100: Fx-calib spectrum (w/telluric)\n",
    "    * 640 / 110: Fx-calib spectrum (w/telluric)\n",
    "    * 666 / 100: Flux-calibrated spectrum - extract_star\n",
    "    * 666 / 110: Flux-calibrated background - extract_star\n",
    "    * 670 / 118: check_flux plot - extract_star\n",
    "* **Comments**:\n",
    "    * Here are examples of [.sh](http://snf.in2p3.fr/file/34002334/content/), [.err](http://snf.in2p3.fr/file/34095170/content/) and [.out](http://snf.in2p3.fr/file/34095166/content/) files of a [typical PMS job](http://snf.in2p3.fr/job/254508/).\n",
    "    * Usually used as followed\n",
    "    \n",
    "            plan_flux_calibration -p SNF-0203-NEWYORK LSQ12cyz --multistd -m SNF-0203-NEWYORK \n",
    "                                  -j SNF-0203-NEWYORK --truncateR 5100,9700 --allowRelative --outver 203\n",
    "            \n",
    "* **CVS link**: [plan_flux_calibration.py](https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/plan_flux_calibration.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_gs_psf` !!\n",
    "\n",
    "* **Purpose**: Generate PSFs for DDT extraction using Gerard's code (estimation from P channel). \n",
    "* **Who**: Who should run it.\n",
    "* **When**: When to run it.\n",
    "* **Where**: Where to run it.\n",
    "* **What**: What does it do/run/produce.\n",
    "* **On**: What to run it on (target, night, etc.).\n",
    "* **DB**: Does it need a DB interaction (YES or NO).\n",
    "* **Dependencies**: List of the main non-standard dependencies.\n",
    "* **Inputs**:\n",
    "    * 18 / 0: Preprocessed object frame\n",
    "* **Outputs**:\n",
    "    * 210 / 0: MF stars measurements from Gerard (ES-PSF image_fit)\n",
    "    * 211 / 0: ExtractStar PSF prediction (ES-PSF) [yaml]\n",
    "* **Comments**: Any other useful comments on the script/step.\n",
    "* **CVS link**: Pointer to the online CVS link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plan_cubefit` !\n",
    "\n",
    "* **Purpose**: Host-galaxy subtraction.\n",
    "* **Who**:  Person in charge of the production\n",
    "* **When**: After `plan_flux_calibration`\n",
    "* **Where**: Under /afs/in2p3.fr/group/snovae/snprodJob/VERSION/MoreFlux/CUBEFIT\n",
    "* **What**:\n",
    "    * Runs cubefit, cubefit-plot and cubefit-subtract on a set of selected flux-calibrated cubes\n",
    "* **On**: A set of targets.\n",
    "* **DB**: YES.\n",
    "* **Dependencies**: List of the main non-standard dependencies.\n",
    "* **Inputs**:\n",
    "    * 23 / 12: Telluric-corrected, flux-calibrated cube [3D]\n",
    "    * 211 / 0: ExtractStar PSF prediction (ES-PSF) [yaml]\n",
    "* **Outputs**:\n",
    "    * 250 / 820: Cubefit - Full galaxy model + Per-epoch results [fits]\n",
    "    * 250 / 829: Cubefit - JSON configuration file [json]\n",
    "    * 260 / 820: Cubefit - flux-calibrated host-subtracted cube [fits]\n",
    "    * 666 / 850: Cubefit - Flux-calibrated spectrum\n",
    "* **Comments**: None\n",
    "* **CVS link**: https://github.com/snfactory/cubefit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-set pre-analysis and packaging\n",
    "\n",
    "### `plan_analyse_timeseries` !!\n",
    "\n",
    "* **Purpose**: \n",
    "* **Who**: Who should run it.\n",
    "* **When**: When to run it.\n",
    "* **Where**: Where to run it.\n",
    "* **What**: What does it do/run/produce.\n",
    "* **On**: What to run it on (target, night, etc.).\n",
    "* **DB**: Does it need a DB interaction (YES or NO).\n",
    "* **Dependencies**: List of the main non-standard dependencies.\n",
    "* **Inputs**: XFclass + 800 for cubefit productions\n",
    "    * 666 / 100: Flux-calibrated spectrum - extract_star\n",
    "* **Outputs**: XFclass + 800 for cubefit productions\n",
    "    * 670 / 100 : Merged flux-calibrated spectrum - extract_star\n",
    "    * 680 / 100: Synth photometry - SNF    - extract_star\n",
    "    * 680 / 101: Synth photometry - Bessel - extract_star\n",
    "    * 680 / 102: Synth photometry - SDSS   - extract_star\n",
    "    * 680 / 103: Synth photometry - SNLS   - extract_star\n",
    "    * 680 / 104: Synth photometry - KAIT   - extract_star\n",
    "    * 680 / 105: Synth photometry - CSP    - extract_star\n",
    "    * 680 / 106: Synth photometry - CfA    - extract_star\n",
    "    * 680 / 117: Stdstar check_specs plot (w/telluric) - extract_star\n",
    "    * 680 / 118: Stdstar check_specs plot - extract_star\n",
    "    * 680 / 119: Spectral timeseries plot  - extract_star\n",
    "    * 700 / 100: SALT2 fit results YAML file          - extract_star\n",
    "* **Comments**: Any other useful comments on the script/step.\n",
    "* **CVS link**: Pointer to the online CVS link.\n",
    "\n",
    "### `study_flux_quality` !!\n",
    "\n",
    "check the magnitude dispersion for StdStars and SALT2 fits for SNe Ia. Build the *good*/*bad* lists.\n",
    "\n",
    "### `update_idr_config` !!\n",
    "\n",
    "use this script to create the CONFIG.yaml file from an old IDR\n",
    "\n",
    "### `build_idr` !!\n",
    "\n",
    "build an IDR from a given production name and list of targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "The libraries and scripts described below are used in the differents steps of the pipeline.\n",
    "\n",
    "### `record_timestamp`\n",
    "\n",
    "* **Purpose**: Sends a command-line/timestamp to yaml, stdout and stderr\n",
    "* **Who**: Run in plan_\\* shell scripts\n",
    "* **When**: While running a job at CC\n",
    "* **Where**: Usually runs on a worker\n",
    "* **What**: \n",
    "    * Take a yaml file and a message as input              \n",
    "    * Usage: \n",
    "    \n",
    "            record_timestamp THEYAML THEMESSAGE\n",
    "        \n",
    "    * Append the input message into the yaml as followed\n",
    "            \n",
    "            - type : record_timestamp\n",
    "              utc : UTCDATE\n",
    "              message : THEINPUTMESSAGE\n",
    "              \n",
    "* **On**: A yaml file\n",
    "* **DB**: NO\n",
    "* **Dependencies**: None\n",
    "* **Inputs**: A yaml log\n",
    "* **Outputs**: A modified yaml log\n",
    "* **Comments**: None\n",
    "* **CVS link**: https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/record_timestamp.py\n",
    "\n",
    "### `update_job`\n",
    "\n",
    "* **Purpose**: Send a message to the DB about job progress.\n",
    "* **Who**: Run in plan_\\* shell scripts\n",
    "* **When**: While running a job at CC\n",
    "* **Where**: Usually runs on a worker\n",
    "* **What**: \n",
    "    * If running in batch, the job Name will be taken from the environment, as well than extra parameter (e.g., log file)\n",
    "    * Take the input status (Starting or Ending) and update the Job DB table accordingly\n",
    "    * Usage: \n",
    "        \n",
    "            update_job -s Starting -f AN_AUXFILE_TO_REGISTER (usualy the yaml log)\n",
    "            update_job -s Ending\n",
    "            \n",
    "* **On**: None\n",
    "* **DB**: YES\n",
    "* **Dependencies**: \n",
    "    * SnfWrite.SnfJob\n",
    "    * SnfMonitor.Monitor\n",
    "* **Inputs**: \n",
    "    * A job status (a string, option -s)\n",
    "    * Must be \"Starting\", \"Ending\", \"FromJob\", or \"Ended\"\n",
    "* **Outputs**: None\n",
    "* **Comments**: None\n",
    "* **CVS link**: https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/update_job.py\n",
    "\n",
    "### `record_outcome`\n",
    "\n",
    "* **Purpose**: Record the outcome message of a script into the yaml log file\n",
    "* **Who**: Run in plan_\\* shell scripts\n",
    "* **When**: While running a job at CC\n",
    "* **Where**: Usually runs on a worker\n",
    "* **What**:\n",
    "    \n",
    "    * Usage: \n",
    "    \n",
    "            record_outcome YAML_LOG THE_ACTION THE_EXIT_CODE_MESSAGE\n",
    "            \n",
    "    * The exit code message is catched by a $?\n",
    "    * The action is usually of the form `nameofthescript@nameofthefile`\n",
    "    * Append the input message into the yaml as followed\n",
    "            \n",
    "            - type : record_outcome\n",
    "              action : THE_ACTION\n",
    "              message : THE_SCRIPT_OUTCOME\n",
    "     \n",
    "* **On**: A yaml log.\n",
    "* **DB**: NO.\n",
    "* **Dependencies**: None.\n",
    "* **Inputs**: A yaml log.\n",
    "* **Outputs**: An updated yaml log.\n",
    "* **Comments**: None\n",
    "* **CVS link**: https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/record_outcome.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `record_processing`\n",
    "\n",
    "* **Purpose**: Records processing info for the DB.\n",
    "* **Who**: Run in plan_\\* shell scripts\n",
    "* **When**: While running a job at CC\n",
    "* **Where**: Usually runs on a worker\n",
    "* **What**: \n",
    "    * Usage: \n",
    "    \n",
    "            record_processing --outver THE_OUT_DB_VERSION \\\n",
    "                              YAML_LOG \\\n",
    "                              THE_FCLASS \\ \n",
    "                              MAIN_INPUT \\\n",
    "                              MAINOUTPUT:XFCLASS \\\n",
    "                              OTHEROUTPUT:XFCLASS ...\n",
    "                              \n",
    "    * Append the input info into the yaml as followed\n",
    "        \n",
    "            - type : record_processing\n",
    "              fclass : THE_FCLASS\n",
    "              version : THE_OUT_DB_VERSION\n",
    "              main_in_file : MAIN_INPUT\n",
    "              other_in_file : []\n",
    "              main_out_file : \n",
    "                MAINOUTPUT : XFCLASS\n",
    "              other_out_file : \n",
    "                OTHEROUTPUT : XFCLASS\n",
    "                ...\n",
    "    \n",
    "* **On**: A yaml log file\n",
    "* **DB**: NO.\n",
    "* **Dependencies**: None.\n",
    "* **Inputs**: A yaml log file, the main input and main output files\"\n",
    "* **Outputs**: An updated yaml log file\n",
    "* **Comments**: None\n",
    "* **CVS link**: https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/record_processing.py\n",
    "\n",
    "### `record_quality`\n",
    "\n",
    "* **Purpose**: Record in the yaml log a flag given to a input process\n",
    "* **Who**: Run in plan_\\* shell scripts\n",
    "* **When**: While running a job at CC\n",
    "* **Where**: Usually runs on a worker\n",
    "* **What**: \n",
    "    * Usage: \n",
    "            \n",
    "            record_quality -i YAML_LOG -f FILE_TO_FLAG -w THE_FLAG\n",
    "            \n",
    "    * Append the input flag into the yaml as followed\n",
    "    \n",
    "            - type : record_quality          \n",
    "              filename: FILE_TO_FLAG\n",
    "              quality: 2\n",
    "              quality_s: THE_FLAG\n",
    "    \n",
    "    * The warning message must be a flag of the official list of flag (that can be found [here](Flagging-system.html#official-list-of-flags))\n",
    "* **On**: A yaml log file\n",
    "* **DB**: NO\n",
    "* **Dependencies**: \n",
    "    * `libRecord` (SNFactory/Tasks/Processing/scripts/)\n",
    "        * depends on the bitarray python library\n",
    "* **Inputs**: A yaml log file\n",
    "* **Outputs**: An updated yaml log file\n",
    "* **Comments**: None\n",
    "* **CVS link**: https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/record_quality.py\n",
    "\n",
    "### `accountant`\n",
    "\n",
    "* **Purpose**: Do accounting on records produced by a pipeline.\n",
    "* **Who**: Run in plan_\\* shell scripts\n",
    "* **When**: While running a job at CC\n",
    "* **Where**: Usually runs on a worker\n",
    "* **What**: \n",
    "* **On**: A yaml log file\n",
    "* **DB**: YES\n",
    "* **Dependencies**: \n",
    "    * SnfWrite (SnfFName, SnfRegisterByFile, SRBbulkRegister, SnfJob) \n",
    "    * Django DB models: from processing.process.models import File, Pose, Exposure\n",
    "    * SnfPhotPipeline.IO (see [plan_photometric_ratios](SNfCodeDoc.html#plan-photometric-ratios))\n",
    "* **Inputs**: A yaml log file\n",
    "* **Outputs**: Info saved in the DB. Data copied on disk at CC.\n",
    "* **Comments**: None\n",
    "* **CVS link**: https://cvs.in2p3.fr/snovae-SNFactory/Tasks/Processing/scripts/accountant.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SnfObj` scripts !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
